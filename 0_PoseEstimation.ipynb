{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !! WARNING !! Before you continue... \n",
    "\n",
    "1. Make sure you've made a copy in your Drive (you can't save otherwise)\n",
    "2. Make sure you've changed your kernel type to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup your linux environment on Google Colab\n",
    "\n",
    "!sudo apt install python3.8-full python3-pip\n",
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\n",
    "!python --version\n",
    "\n",
    "!pip3 install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install dependencies\n",
    "!pip install gdown\n",
    "!pip install ipykernel\n",
    "!pip install openmim \n",
    "!mim install mmengine \n",
    "!pip install mmcv==2.1.0 -f https://download.openmmlab.com/mmcv/dist/cu121/torch2.1/index.html\n",
    "!mim install \"mmdet==3.1.0\" --quiet\n",
    "\n",
    "!git clone https://github.com/open-mmlab/mmpose.git\n",
    "%cd mmpose\n",
    "!pip install -r requirements.txt\n",
    "!pip install -v -e .\n",
    "# \"-v\" means verbose, or \n",
    "\n",
    "!mim install \"mmpose>=1.1.0\" --quiet\n",
    "!mim install \"mmpretrain>=1.0.0rc8\" #needed for video inference with pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Check that your install is working\n",
    "\n",
    "!mim download mmpose --config td-hm_hrnet-w48_8xb32-210e_coco-256x192  --dest .\n",
    "\n",
    "!python demo/image_demo.py \\\n",
    "    tests/data/coco/000000000785.jpg \\\n",
    "    td-hm_hrnet-w48_8xb32-210e_coco-256x192.py \\\n",
    "    td-hm_hrnet-w48_8xb32-210e_coco-256x192-0e67c616_20220913.pth \\\n",
    "    --out-file vis_results.jpg \\\n",
    "    --draw-heatmap\n",
    "\n",
    "\n",
    "# show image \n",
    "from IPython.display import Image\n",
    "Image(\"vis_results.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Now let's try inference on a video\n",
    "import os \n",
    "from IPython.display import Video\n",
    "\n",
    "video_file = \"/content/mmpose/demo/resources/demo.mp4\"\n",
    "pred_out_dir = \"/content/mmpose/demo/pred_out\"\n",
    "vis_out_dir = \"/content/mmpose/demo/vis_out\"\n",
    "\n",
    "os.makedirs(pred_out_dir, exist_ok=True)\n",
    "os.makedirs(vis_out_dir, exist_ok=True)\n",
    "\n",
    "!python ./demo/inferencer_demo.py {video_file} \\\n",
    "    --pose2d vitpose-h --pred-out-dir {pred_out_dir} \\\n",
    "    --vis-out-dir {vis_out_dir} --skeleton-style openpose --radius 5 --thickness 3 #--black-background= f\"\"\n",
    "\n",
    "# Path to your video file\n",
    "Video(f'{vis_out_dir}/demo.mp4', embed=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title now let's see how this works on a few different videos and discuss the results\n",
    "videos_path = \"/content/videos\"\n",
    "os.makedirs(videos_path, exist_ok=True)\n",
    "\n",
    "!gdown --folder https://drive.google.com/drive/folders/1F9TJ0f44B0_69xG35jMKBzk-nVNOHpiC?usp=share_link --output {videos_path}\n",
    "\n",
    "for video_file in os.listdir(videos_path):\n",
    "    video_path = os.path.join(videos_path, video_file)\n",
    "\n",
    "    !python ./demo/inferencer_demo.py {video_path} \\\n",
    "                --pose2d vitpose-h --pred-out-dir {pred_out_dir} \\\n",
    "                --vis-out-dir {vis_out_dir} --skeleton-style openpose --radius 5 --thickness 3 #--black-background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Now let's try a few different pre-trained models!\n",
    "\n",
    "# things to try: \n",
    "# pose2d human \n",
    "# pose2d wholebody\n",
    "# pose3d human3d\n",
    "\n",
    "import os \n",
    "from IPython.display import Video\n",
    "\n",
    "model_test = \"human\"\n",
    "video_file = \"demo.mp4\"\n",
    "\n",
    "video_path = \"/content/mmpose/demo/resources/\"\n",
    "pred_out_dir = f\"/content/mmpose/demo/{model_test}/pred_out\"\n",
    "vis_out_dir = f\"/content/mmpose/demo/{model_test}/vis_out\"\n",
    "\n",
    "os.makedirs(pred_out_dir, exist_ok=True)\n",
    "os.makedirs(vis_out_dir, exist_ok=True)\n",
    "\n",
    "!python ./demo/inferencer_demo.py {os.path.join(video_path,video_file)} \\\n",
    "    --pose2d vitpose-h --pred-out-dir {pred_out_dir} \\\n",
    "    --vis-out-dir {vis_out_dir} --skeleton-style openpose --radius 5 --thickness 3 #--black-background= f\"\"\n",
    "\n",
    "# Path to your video file\n",
    "Video(f'{vis_out_dir}/{video_file}', embed=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title What metrics can we derive from the timeseries? \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
