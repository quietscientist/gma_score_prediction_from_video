{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ngtR6tOyIFM-",
    "outputId": "12e32194-6ae4-45c0-f4e5-3e1a3d8c66f8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys, os, cv2, glob, json, gc\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from moviepy.editor import VideoFileClip\n",
    "import skvideo.io\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import circstat as CS\n",
    "import scipy as sc\n",
    "import math, random\n",
    "\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from processing import *\n",
    "from kinematics import *\n",
    "from skeleton import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.makedirs('./data',exist_ok=True)\n",
    "os.makedirs('./utils',exist_ok=True)\n",
    "\n",
    "#download custom processing scripts if not already downloaded\n",
    "# !wget -P . https://raw.githubusercontent.com/quietscientist/gma_score_prediction_from_video/refs/heads/main/utils/kinematics.py\n",
    "# !wget -P . https://raw.githubusercontent.com/quietscientist/gma_score_prediction_from_video/refs/heads/main/utils/circstat.py\n",
    "# !wget -P . https://raw.githubusercontent.com/quietscientist/gma_score_prediction_from_video/refs/heads/main/utils/processing.py\n",
    "# !wget -P . https://raw.githubusercontent.com/quietscientist/gma_score_prediction_from_video/refs/heads/main/utils/skeleton.py\n",
    "\n",
    "#download example data or upload your own json annotations\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# Download example raw data from figshare or specify path to your own json annotations |\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "#!wget -P . https://figshare.com/ndownloader/articles/25316500/versions/1\n",
    "# #unzip data into ./data folder and remove zip file\n",
    "# !unzip ./1 -d ./data\n",
    "# !rm ./1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pjaLpviIFNA",
    "outputId": "25749e58-269a-48d4-aa72-667fc737ce05"
   },
   "outputs": [],
   "source": [
    "#uncomment install if running on google colab\n",
    "#%pip install scikit-video\n",
    "DEIDENTIFY = True #set to True to deidentify data, and exclude xy coordinates and pixel values from output\n",
    "OVERWRITE = True\n",
    "USE_CENTER_INSTANCE = False\n",
    "USE_BEST_INSTANCE = True\n",
    "\n",
    "dataset = 'CHOP'\n",
    "json_path = f'./data/Infant Pose Data/{dataset}/annotations'\n",
    "json_files = os.listdir(json_path)\n",
    "directory = f'./data'\n",
    "\n",
    "save_path = f'./pose_estimates/{dataset}_pose_estimates'\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "kp_mapping = {0:'Nose', 1:'Neck', 2:'RShoulder', 3:'RElbow', 4:'RWrist', 5:'LShoulder', 6:'LElbow',\n",
    "              7:'LWrist', 8:'RHip', 9:'RKnee', 10:'RAnkle', 11:'LHip',\n",
    "              12:'LKnee', 13:'LAnkle', 14:'REye', 15:'LEye', 16:'REar', 17:'LEar'}\n",
    "\n",
    "# Define the DataFrame columns as specified\n",
    "columns = ['video_number', 'video', 'bp', 'frame', 'x', 'y', 'c','fps', 'pixel_x', 'pixel_y', 'time', 'part_idx']\n",
    "data = []  # This will hold the data to be loaded into the DataFrame\n",
    "\n",
    "vid_info = pd.read_csv(f'./data/{dataset}_video_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ7Dz7hYfoAg",
    "outputId": "dd4ea572-f5c7-4660-b7d2-f07381427692"
   },
   "outputs": [],
   "source": [
    "# format files as pkl with openpose standard and bodypart labels\n",
    "\n",
    "def process_file(args):\n",
    "    \"\"\"Function to process a single file.\"\"\"\n",
    "    file_number, file, json_path, save_path, vid_info, kp_mapping = args\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(json_path, file)\n",
    "    fname = file.split('.')[0]\n",
    "    interim = []\n",
    "\n",
    "    if not OVERWRITE and os.path.exists(f'{save_path}/{fname}.pkl'):\n",
    "        return\n",
    "\n",
    "    # Open and load the JSON data\n",
    "    try: \n",
    "        with open(file_path, 'r') as f:\n",
    "            frames = json.load(f)\n",
    "            info = vid_info[vid_info['video'] == fname]\n",
    "            fps = vid_info['fps'].values[0]\n",
    "\n",
    "            pixel_x = vid_info['width'].values[0]\n",
    "            pixel_y = vid_info['height'].values[0]\n",
    "            \n",
    "            center_x = pixel_x / 2\n",
    "            center_y = pixel_y / 2\n",
    "            \n",
    "            # Iterate through each frame in the JSON file\n",
    "            for frame in frames:\n",
    "                frame_id = frame['frame_id']\n",
    "                if 'instances' in frame and len(frame['instances']) > 0:\n",
    "\n",
    "                    if USE_CENTER_INSTANCE:\n",
    "                        instance_id = get_center_instance(frame['instances'], center_x, center_y)\n",
    "                    elif USE_BEST_INSTANCE:\n",
    "                        instance_id = get_best_instance(frame['instances'])\n",
    "                    else:\n",
    "                        instance_id = 0\n",
    "\n",
    "                    keypoints = frame['instances'][instance_id]['keypoints']\n",
    "                    confidence = frame['instances'][instance_id]['keypoint_scores']\n",
    "                    keypoints, confidence = convert_coco_to_openpose(keypoints, confidence)\n",
    "\n",
    "                    # Iterate through each keypoint\n",
    "                    for part_idx, (x, y) in enumerate(keypoints):\n",
    "\n",
    "                        bp = kp_mapping[part_idx]\n",
    "                        fps = fps\n",
    "                        time = frame_id / fps\n",
    "                        c = confidence[part_idx]\n",
    "\n",
    "                        row = [file_number, fname, bp, frame_id, x, y, c, fps, pixel_x, pixel_y, time, part_idx]\n",
    "                        interim.append(row)\n",
    "\n",
    "        interim_df = pd.DataFrame(interim, columns=columns)\n",
    "        interim_df.to_csv(f'{save_path}/{fname}.csv', index=False)\n",
    "\n",
    "        del interim_df\n",
    "        return\n",
    "    \n",
    "    except Exception as e:\n",
    "        return\n",
    "    \n",
    "def process_annotations_multiprocess(json_files, json_path, save_path, vid_info, kp_mapping):\n",
    "    \"\"\"Run the annotation processing using multiprocessing.\"\"\"\n",
    "    args = [\n",
    "        (file_number, file, json_path, save_path, vid_info, kp_mapping)\n",
    "        for file_number, file in enumerate(json_files)\n",
    "    ]\n",
    "\n",
    "    # Set up a pool of workers\n",
    "    with Pool(processes=20) as pool:\n",
    "        pool.map(process_file, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_annotations_multiprocess(json_files, json_path, save_path, vid_info, kp_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gF9SA7KO1jbQ",
    "outputId": "0f87b278-d65f-4fba-9149-bd36844f6390"
   },
   "outputs": [],
   "source": [
    "# Ensure the save_path directory exists\n",
    "save_path = f'./pose_estimates/{dataset}_norm'\n",
    "\n",
    "if os.path.exists(f'{save_path}/pose_estimates_{dataset}.csv'):\n",
    "    os.remove(f'{save_path}/pose_estimates_{dataset}.csv')\n",
    "    print('Removed existing CSV file')\n",
    "\n",
    "for pklfile in tqdm(os.listdir(save_path)):\n",
    "    if not pklfile.endswith('.pkl'):\n",
    "        continue\n",
    "    else:\n",
    "        interim_df = pd.read_pickle(f'{save_path}/{pklfile}')\n",
    "        interim_df.to_csv(f'{save_path}/pose_estimates_{dataset}.csv', mode='a', header=False, index=False)\n",
    "\n",
    "    del interim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERK1DQ-92DsG"
   },
   "outputs": [],
   "source": [
    "csv_path = f'{save_path}/pose_estimates_{dataset}.csv'\n",
    "output_csv_path = f'{save_path}/pose_estimates_{dataset}_b.csv'\n",
    "chunksize = 1000  # Number of rows per chunk\n",
    "\n",
    "# Define the new headers\n",
    "new_headers = ['video_number', 'video', 'bp', 'frame', 'x', 'y', 'c', 'fps', 'pixel_x', 'pixel_y', 'time', 'part_idx']\n",
    "\n",
    "# Read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_path, chunksize=chunksize)\n",
    "\n",
    "# Process the first chunk\n",
    "first_chunk = next(chunk_iterator)\n",
    "first_chunk.columns = new_headers\n",
    "first_chunk.to_csv(output_csv_path, mode='w', index=False)\n",
    "\n",
    "# Process the rest of the chunks and append them to the new CSV file without headers\n",
    "for chunk in chunk_iterator:\n",
    "    chunk.columns = new_headers\n",
    "    chunk.to_csv(output_csv_path, mode='a', index=False, header=False)\n",
    "\n",
    "# rename the csv file\n",
    "os.rename(csv_path, f'{save_path}/pose_estimates_{dataset}_x.csv')\n",
    "os.rename(output_csv_path, csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I4107PvX25fK"
   },
   "outputs": [],
   "source": [
    "# Smooth detections and compute features\n",
    "pose_estimate_path = f'./pose_estimates/{dataset}_pose_estimates'\n",
    "csv_path = f'{pose_estimate_path}/pose_estimates_{dataset}.csv'\n",
    "save_path = f'{pose_estimate_path}/pose_estimates_{dataset}_processed.csv'\n",
    "\n",
    "# List of subdirectories to create\n",
    "subdirs = [\n",
    "    \"\",\n",
    "    \"xdf\",\n",
    "    \"adf\",\n",
    "    \"xy_features\",\n",
    "    \"angle_features\",\n",
    "    \"xy_features/total\",\n",
    "    \"angle_features/total\",\n",
    "    \"xy_features/windows\",\n",
    "    \"angle_features/windows\",\n",
    "    \"smooth\",\n",
    "    \"anim\"\n",
    "]\n",
    "\n",
    "# Create necessary directories\n",
    "for subdir in subdirs:\n",
    "    os.makedirs(f'{pose_estimate_path}/{subdir}', exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "H5A-LkovEMav"
   },
   "outputs": [],
   "source": [
    "def process_dataframe(file):\n",
    "    df = pd.read_csv(os.path.join(pose_estimate_path, file))\n",
    "\n",
    "    if df.empty:\n",
    "        #print(\"DataFrame is empty, skipping processing.\")\n",
    "        return\n",
    "    # print(f\"Processing DataFrame for video_number: {df['video_number'].iloc[0]}\")\n",
    "    try:\n",
    "        if dataset == 'Youtube':\n",
    "            session = df['video'].unique()[0].split('_')[1][0]\n",
    "            infant = df['video'].unique()[0].split('_')[1][3:]\n",
    "            age = '3Month'\n",
    "        elif dataset == 'Clinical':\n",
    "            # split based on naming convention\n",
    "            session = df['video'].unique()[0].split('_')[1][1]\n",
    "            infant = df['video'].unique()[0].split('_')[0][-1]\n",
    "            age = '3Month'\n",
    "        elif dataset == 'gma_score_prediction': \n",
    "            session = 0\n",
    "            infant = df['video'].unique()[0]\n",
    "            age = '3Month'\n",
    "        elif dataset == 'CHOP': \n",
    "            session = df['video'].unique()[0].split('_')[1]\n",
    "            infant = df['video'].unique()[0].split('_')[0]\n",
    "            age = df['video'].unique()[0].split('_')[2]\n",
    "        \n",
    "        # print(f'infant: {infant} {session} {age}')\n",
    "\n",
    "\n",
    "        median_window = 1\n",
    "        mean_window = 1\n",
    "        delta_window = 0.25  # Smoothing applied to delta_x, velocity, acceleration\n",
    "\n",
    "        df['x'] = pd.to_numeric(df['x'])\n",
    "        df['y'] = pd.to_numeric(df['y'])\n",
    "\n",
    "        #filter low confidence detections\n",
    "        #df = df[df['c'] > 0.5]\n",
    "\n",
    "        # Interpolate\n",
    "        df = df.groupby(['video', 'bp']).apply(interpolate_df).reset_index(drop=True)\n",
    "\n",
    "        # Median and mean filter\n",
    "        median_window = 0.5\n",
    "        mean_window = 0.5\n",
    "        df = df.groupby(['video', 'bp']).apply(lambda x: smooth(x, 'y', median_window, mean_window)).reset_index(drop=True)\n",
    "        df = df.groupby(['video', 'bp']).apply(lambda x: smooth(x, 'x', median_window, mean_window)).reset_index(drop=True)\n",
    "        \n",
    "        df = normalise_skeletons(df) \n",
    "        # df.to_csv(f'{pose_estimate_path}/smooth/{infant}_{session}_{age}_smooth_norm.csv')\n",
    "    \n",
    "    except:\n",
    "        f'could not process video {df[\"video\"].unique()[0]}'\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Rotate and normalise by reference\n",
    "        xdf = get_dynamics_xy(df, delta_window)    \n",
    "        xdf.to_csv(f'{pose_estimate_path}/xdf/{infant}_{session}_{age}_smooth_norm_xy.csv')\n",
    "\n",
    "        adf = get_joint_angles(df)\n",
    "        adf = get_dynamics_angle(adf, delta_window)\n",
    "        adf.to_csv(f'{pose_estimate_path}/adf/{infant}_{session}_{age}_smooth_norm_ang.csv')\n",
    "\n",
    "    except KeyError as e:\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2979 files\n"
     ]
    }
   ],
   "source": [
    "pose_estimate_files = os.listdir(pose_estimate_path)\n",
    "pose_estimate_files = [file for file in pose_estimate_files if file.endswith('.csv')]\n",
    "\n",
    "# pose_estimate_files = random.sample(pose_estimate_files, 10)\n",
    "\n",
    "print(f'Processing {len(pose_estimate_files)} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(processes=25) as pool:\n",
    "    pool.map(process_dataframe, pose_estimate_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0],\n",
    "          [0, 255, 0], \\\n",
    "          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255],\n",
    "          [85, 0, 255], \\\n",
    "          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85],[255, 0, 0]]\n",
    "\n",
    "limbSeq = [[2, 3], [2, 6], [3, 4], [4, 5], [6, 7], [7, 8], [2, 9], [9, 10], \\\n",
    "           [10, 11], [2, 12], [12, 13], [13, 14], [2, 1], [1, 15], [15, 17], \\\n",
    "           [1, 16], [16, 18]] #[3, 17], [6, 18]]\n",
    "\n",
    "def plot_skel(df, ax, xvar, yvar):\n",
    "    alpha = 0.3\n",
    "\n",
    "    for i, limb in enumerate(limbSeq):\n",
    "        l1 = limb[0] - 1\n",
    "        l2 = limb[1] - 1\n",
    "        df_l1 = df[df.part_idx == l1]\n",
    "        df_l2 = df[df.part_idx == l2]\n",
    "        if not df_l1.empty and not df_l2.empty:\n",
    "            ax.plot(\n",
    "                [df_l1[xvar].iloc[0], df_l2[xvar].iloc[0]],\n",
    "                [df_l1[yvar].iloc[0], df_l2[yvar].iloc[0]],\n",
    "                linewidth=5,\n",
    "                color=[j / 255 for j in colors[i]],\n",
    "                alpha=alpha,\n",
    "            )\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        ax.plot(\n",
    "            df.iloc[i][xvar],\n",
    "            df.iloc[i][yvar],\n",
    "            'o',\n",
    "            markersize=10,\n",
    "            color=[j / 255 for j in colors[i]],\n",
    "            alpha=alpha,\n",
    "        )\n",
    "\n",
    "\n",
    "def animate_coordinates_with_skeleton(file):\n",
    "    \n",
    "    file = f'{pose_estimate_path}/smooth/{file}'\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    fname = os.path.basename(file)\n",
    "\n",
    "    fps = df.fps[0]\n",
    "    framen = df.frame.max()\n",
    "    frame_interval = 5\n",
    "    dpi = 80\n",
    "\n",
    "    output_gif = f'{pose_estimate_path}/anim/{os.path.splitext(fname)[0]}.gif'\n",
    "    print(output_gif)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6), dpi=dpi)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "    ax.invert_yaxis()\n",
    "    ax.axis('off')\n",
    "\n",
    "    def update(frame_idx):\n",
    "        ax.clear()\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        ax.set_xlim(-3, 3)\n",
    "        ax.set_ylim(-3, 3)\n",
    "        ax.invert_yaxis()\n",
    "        ax.axis('off')\n",
    "        plot_skel(df[df.frame == frame_idx], ax, 'x', 'y')\n",
    "\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig,\n",
    "        update,\n",
    "        frames=range(0, framen, frame_interval),\n",
    "        interval=int(1 / fps * 1000 * frame_interval),\n",
    "        repeat_delay=1000\n",
    "    )\n",
    "\n",
    "    ani.save(output_gif, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "\n",
    "    del ani, df, fig, ax\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_files = os.listdir(f'{pose_estimate_path}/smooth')\n",
    "smooth_files = [file for file in smooth_files if file.endswith('.csv')]\n",
    "\n",
    "output_names = os.listdir(f'{pose_estimate_path}/anim')\n",
    "output_names = [file for file in output_names if file.endswith('.gif')]\n",
    "\n",
    "match = []\n",
    "for name in output_names:\n",
    "    parts = name.split('.')[0]\n",
    "    match.append(f'{parts}.csv')\n",
    "\n",
    "smooth_files = [file for file in smooth_files if file not in match]\n",
    "#smooth_files = random.sample(smooth_files, 5)\n",
    "\n",
    "print(len(smooth_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(processes=25) as pool:\n",
    "    pool.map(animate_coordinates_with_skeleton, smooth_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_files = os.listdir(f'{pose_estimate_path}/smooth')\n",
    "smooth_files = [file for file in smooth_files if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributions to check for outliers\n",
    "def get_number_of_frames(file):\n",
    "    # Read only necessary columns to reduce memory usage\n",
    "    df = pd.read_csv(file, usecols=['video', 'frame'])\n",
    "    \n",
    "    # Extract unique video info (assumes consistent naming convention)\n",
    "    video_info = str(df['video'].iloc[0]).split('_')  # Only fetch the first row\n",
    "    infant = video_info[0]\n",
    "    session = video_info[1]\n",
    "    age = video_info[2]\n",
    "    \n",
    "    # Get the maximum frame number\n",
    "    n = df['frame'].max()\n",
    "    \n",
    "    # Return the result as a dictionary\n",
    "    return {\n",
    "        'infant': infant,\n",
    "        'session': session,\n",
    "        'age': age,\n",
    "        'n_frames': n\n",
    "    }\n",
    "\n",
    "def gather_frame_counts(files):\n",
    "    for file in files:\n",
    "        file_path = f'{pose_estimate_path}/smooth/{file}'\n",
    "        yield get_number_of_frames(file_path)  # Use a generator for memory efficiency\n",
    "\n",
    "\n",
    "smooth_files = os.listdir(f'{pose_estimate_path}/smooth')\n",
    "smooth_files = [file for file in smooth_files if file.endswith('.csv')]\n",
    "\n",
    "frame_counts = list(gather_frame_counts(smooth_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{pose_estimate_path}/distributions', exist_ok=True)\n",
    "\n",
    "#frame_counts_df = pd.DataFrame(frame_counts)\n",
    "#frame_counts_df.to_csv(f'{pose_estimate_path}/distributions/frame_counts.csv', index=False)\n",
    "frame_counts_df = pd.read_csv(f'{pose_estimate_path}/distributions/frame_counts.csv')\n",
    "\n",
    "# Define custom colors for each age\n",
    "age_colors = {0: '#56B4E9', 1: '#E69F00'}  # Blue and Orange from the CUD palette\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Group by 'age' and overlay histograms with specified colors\n",
    "for age, group in frame_counts_df.groupby('age'):\n",
    "    color = age_colors.get(age, 'gray')  # Default to 'gray' if age is not in age_colors\n",
    "    plt.hist(group['n_frames'], bins=100, alpha=0.5, label=f'Age: {age}', color=color)\n",
    "\n",
    "plt.title('Frame Count Distribution by Age')\n",
    "plt.xlabel('Number of Frames')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'{pose_estimate_path}/distributions/frame_counts.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0ik4gOyCDVZ",
    "outputId": "f16fdcc3-5798-47cc-f78c-282d79d7e7ae"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "chunksize = 100000\n",
    "buffer = pd.DataFrame()\n",
    "\n",
    "# Read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_path, chunksize=chunksize)\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "  unique_videos = chunk['video_number'].unique()\n",
    "\n",
    "  for video_number in unique_videos:\n",
    "      video_chunk = chunk[chunk['video_number'] == video_number]\n",
    "\n",
    "      if not buffer.empty:\n",
    "        if buffer['video_number'].iloc[0] == video_number:\n",
    "\n",
    "            buffer = pd.concat([buffer, video_chunk], ignore_index=True)\n",
    "            if video_number not in chunk['video_number'].values:\n",
    "                process_dataframe(buffer, pose_estimate_path)\n",
    "                buffer = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            process_dataframe(buffer, pose_estimate_path)\n",
    "            buffer = video_chunk\n",
    "      else:\n",
    "          buffer = video_chunk\n",
    "  clear_output(wait=True)\n",
    "  chunk = chunk[~chunk['video_number'].isin(unique_videos)]\n",
    "\n",
    "# # Process any remaining rows in the buffer\n",
    "if not buffer.empty:\n",
    "    print(\"Processing remaining rows in the buffer...\")\n",
    "    process_dataframe(buffer, pose_estimate_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AenvmNmB3cc5",
    "outputId": "7925b160-4c5f-45f2-fe96-c02775cf3106"
   },
   "outputs": [],
   "source": [
    "#compute xy features\n",
    "\n",
    "def process_xdf_file(file): \n",
    "        \n",
    "        xdf = pd.read_csv(os.path.join(f'{pose_estimate_path}/xdf', file))\n",
    "\n",
    "        bps = ['LAnkle', 'RAnkle', 'LWrist', 'RWrist']\n",
    "        filtered_xdf = xdf[np.isin(xdf.bp, bps)]\n",
    "        video_number = xdf.video.unique()[0]\n",
    "        \n",
    "        # Compute window xy features: \n",
    "        mean_type = 'windows'\n",
    "        feature_xy = xdf.groupby(['bp', 'video']).apply(lambda group: rolling_xy_features(group, window_size=60)).reset_index(drop=True)\n",
    "        feature_xy = pd.pivot_table(feature_xy, index=['video','frame'], columns=['bp'])\n",
    "           \n",
    "        l0 = feature_xy.columns.get_level_values(1)\n",
    "        l1 = feature_xy.columns.get_level_values(0)\n",
    "\n",
    "        cols = [l1[i]+'_'+l0[i] for i in range(len(l1))]\n",
    "        feature_xy.columns = cols\n",
    "        feature_xy = feature_xy.reset_index()\n",
    "\n",
    "        # - measure of symmetry (left-right cross correlation)\n",
    "        xdf['dist'] = np.sqrt(xdf['x']**2+xdf['y']**2)\n",
    "        corr_joint = xdf[np.isin(xdf.bp, bps)].groupby(['video', 'part']).apply(lambda x:rolling_corr_lr(x,var='dist')).reset_index()\n",
    "        corr_joint['part'] = 'lrCorr_x_'+corr_joint['part']\n",
    "        corr_joint.drop(columns=['level_2','R','L'],inplace=True)\n",
    "\n",
    "        corr_joint.columns = ['video', 'feature', 'frame', 'Value']\n",
    "        corr_joint = pd.pivot_table(corr_joint, index=['video', 'frame'], columns=['feature'])\n",
    "        l1 = corr_joint.columns.get_level_values(1)\n",
    "        corr_joint.columns = l1\n",
    "        corr_joint = corr_joint.reset_index()\n",
    "        feature_xy = pd.merge(feature_xy, corr_joint, on=['video','frame'], how='outer')\n",
    "\n",
    "        feature_xy.to_csv(f'{pose_estimate_path}/xy_features/{mean_type}/{video_number}_features_{mean_type}_xy.csv', header=True, index=False)\n",
    "    \n",
    "        #compute total xy features (average by video)\n",
    "        mean_type = 'total'\n",
    "\n",
    "        feature_xy = filtered_xdf.groupby(['bp','video']).apply(xy_features).reset_index(drop=True)\n",
    "        feature_xy = pd.pivot_table(feature_xy, index='video', columns=['bp'])\n",
    "        l0 = feature_xy.columns.get_level_values(1)\n",
    "        l1 = feature_xy.columns.get_level_values(0)\n",
    "        cols = [l1[i]+'_'+l0[i] for i in range(len(l1))]\n",
    "        feature_xy.columns = cols\n",
    "        feature_xy = feature_xy.reset_index()\n",
    "\n",
    "        # - measure of symmetry (left-right cross correlation)\n",
    "\n",
    "        xdf['dist'] = np.sqrt(xdf['x']**2+xdf['y']**2)\n",
    "        corr_joint = xdf.groupby(['video', 'part']).apply(lambda x:corr_lr(x,'dist')).reset_index()\n",
    "        corr_joint['part'] = 'lrCorr_x_'+corr_joint['part']\n",
    "        corr_joint.columns = ['video', 'feature', 'Value']\n",
    "        corr_joint = pd.pivot_table(corr_joint, index='video', columns=['feature'])\n",
    "        l1 = corr_joint.columns.get_level_values(1)\n",
    "        corr_joint.columns = l1\n",
    "        corr_joint = corr_joint.reset_index()\n",
    "        feature_xy = pd.merge(feature_xy, corr_joint, on='video', how='outer')\n",
    "        \n",
    "        feature_xy.to_csv(f'{pose_estimate_path}/xy_features/{mean_type}/{video_number}_features_{mean_type}_xy.csv', header=True, index=False)\n",
    "\n",
    "        return\n",
    "\n",
    "# Compute angular features\n",
    "\n",
    "def process_adf_file(file): \n",
    "\n",
    "        adf = pd.read_csv(os.path.join(f'{pose_estimate_path}/adf', file))\n",
    "        \n",
    "        video_number = adf.video.unique()[0]\n",
    "\n",
    "        # Compute window angle features: \n",
    "        mean_type = 'windows'\n",
    "        window_size = 2*int(adf['fps'].iloc[0]) # 2 seconds\n",
    "\n",
    "        # Compute window angle features: \n",
    "        feature_angle = adf.groupby(['bp','video']).apply(rolling_angle_features, window_size=window_size).reset_index(drop=True)\n",
    "        feature_angle = pd.pivot_table(feature_angle, index=['video','frame'], columns=['bp'])\n",
    "        l0 = feature_angle.columns.get_level_values(1)\n",
    "        l1 = feature_angle.columns.get_level_values(0)\n",
    "        cols = [l1[i]+'_'+l0[i] for i in range(len(l1))]\n",
    "        feature_angle.columns = cols\n",
    "        feature_angle =feature_angle.reset_index()\n",
    "\n",
    "        # - measure of symmetry (left-right cross correlation)\n",
    "        corr_joint = adf.groupby(['video', 'part']).apply(rolling_core_lr, window_size=window_size, min_periods=1, var='angle')\n",
    "        corr_joint.reset_index(inplace=True)\n",
    "        corr_joint.drop(columns=['level_2','R','L'],inplace=True)\n",
    "        corr_joint['part'] = 'lrCorr_angle_'+corr_joint['part']\n",
    "        corr_joint.columns = ['video', 'feature', 'frame','Value']\n",
    "        corr_joint = pd.pivot_table(corr_joint, index=['video','frame'], columns=['feature'])\n",
    "        l1 = corr_joint.columns.get_level_values(1)\n",
    "        corr_joint.columns = l1\n",
    "        corr_joint = corr_joint.reset_index()\n",
    "\n",
    "        feature_angle = pd.merge(feature_angle,corr_joint, on=['video','frame'], how='outer')\n",
    "        feature_angle.to_csv(f'{pose_estimate_path}/angle_features/{mean_type}/{video_number}_features_{mean_type}_angle.csv', header=True, index=False)\n",
    "\n",
    "        #compute total angle features (average by video)\n",
    "        mean_type = 'total'\n",
    "        feature_angle = adf.groupby(['bp','video']).apply(angle_features).reset_index(drop=True)\n",
    "        feature_angle = pd.pivot_table(feature_angle, index='video', columns=['bp'])        \n",
    "        \n",
    "        l0 = feature_angle.columns.get_level_values(1)\n",
    "        l1 = feature_angle.columns.get_level_values(0)\n",
    "        cols = [l1[i]+'_'+l0[i] for i in range(len(l1))]\n",
    "        feature_angle.columns = cols\n",
    "        feature_angle =feature_angle.reset_index()\n",
    "\n",
    "        # - measure of symmetry (left-right cross correlation)\n",
    "        corr_joint = adf.groupby(['video', 'part']).apply(lambda x:corr_lr(x,'angle')).reset_index()\n",
    "        corr_joint['part'] = 'lrCorr_angle_'+corr_joint['part']\n",
    "        corr_joint.columns = ['video', 'feature', 'Value']\n",
    "        corr_joint = pd.pivot_table(corr_joint, index='video', columns=['feature'])\n",
    "        l1 = corr_joint.columns.get_level_values(1)\n",
    "        corr_joint.columns = l1\n",
    "        corr_joint = corr_joint.reset_index()\n",
    "        feature_angle = pd.merge(feature_angle,corr_joint, on='video', how='outer')\n",
    "\n",
    "        feature_angle.to_csv(f'{pose_estimate_path}/angle_features/{mean_type}/{video_number}_features_{mean_type}_angle.csv', header=True, index=False)\n",
    "        \n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2977\n"
     ]
    }
   ],
   "source": [
    "xdf_paths = os.listdir(f'{pose_estimate_path}/xdf')  # List of file paths\n",
    "xdf_paths = [file for file in xdf_paths if file.endswith('.csv')]\n",
    "\n",
    "xdf_paths = random.sample(xdf_paths, 5)\n",
    "\n",
    "adf_paths = os.listdir(f'{pose_estimate_path}/adf')  # List of file paths\n",
    "adf_paths = [file for file in adf_paths if file.endswith('.csv')]\n",
    "\n",
    "print(len(xdf_paths), len(adf_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(processes=24) as pool:\n",
    "    pool.map(process_xdf_file, xdf_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with Pool(processes=24) as pool:\n",
    "    pool.map(process_adf_file, adf_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>frame</th>\n",
       "      <th>IQR_acc_x_LAnkle</th>\n",
       "      <th>IQR_acc_x_LElbow</th>\n",
       "      <th>IQR_acc_x_LKnee</th>\n",
       "      <th>IQR_acc_x_LWrist</th>\n",
       "      <th>IQR_acc_x_RAnkle</th>\n",
       "      <th>IQR_acc_x_RElbow</th>\n",
       "      <th>IQR_acc_x_RKnee</th>\n",
       "      <th>IQR_acc_x_RWrist</th>\n",
       "      <th>...</th>\n",
       "      <th>median_y_LAnkle</th>\n",
       "      <th>median_y_LElbow</th>\n",
       "      <th>median_y_LKnee</th>\n",
       "      <th>median_y_LWrist</th>\n",
       "      <th>median_y_RAnkle</th>\n",
       "      <th>median_y_RElbow</th>\n",
       "      <th>median_y_RKnee</th>\n",
       "      <th>median_y_RWrist</th>\n",
       "      <th>lrCorr_x_Ankle</th>\n",
       "      <th>lrCorr_x_Wrist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139_1_0</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.626879</td>\n",
       "      <td>0.036969</td>\n",
       "      <td>1.222065</td>\n",
       "      <td>0.212976</td>\n",
       "      <td>1.471905</td>\n",
       "      <td>-0.047890</td>\n",
       "      <td>1.157515</td>\n",
       "      <td>-0.374298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139_1_0</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.625425</td>\n",
       "      <td>0.036544</td>\n",
       "      <td>1.220938</td>\n",
       "      <td>0.211364</td>\n",
       "      <td>1.471938</td>\n",
       "      <td>-0.049638</td>\n",
       "      <td>1.156081</td>\n",
       "      <td>-0.375767</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139_1_0</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.623971</td>\n",
       "      <td>0.036118</td>\n",
       "      <td>1.219811</td>\n",
       "      <td>0.209752</td>\n",
       "      <td>1.471970</td>\n",
       "      <td>-0.051385</td>\n",
       "      <td>1.154647</td>\n",
       "      <td>-0.377236</td>\n",
       "      <td>-0.976615</td>\n",
       "      <td>-0.996211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139_1_0</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.622707</td>\n",
       "      <td>0.035772</td>\n",
       "      <td>1.218857</td>\n",
       "      <td>0.207982</td>\n",
       "      <td>1.472125</td>\n",
       "      <td>-0.053796</td>\n",
       "      <td>1.153433</td>\n",
       "      <td>-0.378959</td>\n",
       "      <td>-0.988410</td>\n",
       "      <td>-0.978462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139_1_0</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.621444</td>\n",
       "      <td>0.035426</td>\n",
       "      <td>1.217903</td>\n",
       "      <td>0.206211</td>\n",
       "      <td>1.472280</td>\n",
       "      <td>-0.056207</td>\n",
       "      <td>1.152219</td>\n",
       "      <td>-0.380681</td>\n",
       "      <td>-0.991569</td>\n",
       "      <td>-0.953311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     video  frame  IQR_acc_x_LAnkle  IQR_acc_x_LElbow  IQR_acc_x_LKnee  \\\n",
       "0  139_1_0     10               NaN               NaN              NaN   \n",
       "1  139_1_0     11               NaN               NaN              NaN   \n",
       "2  139_1_0     12               NaN               NaN              NaN   \n",
       "3  139_1_0     13               NaN               NaN              NaN   \n",
       "4  139_1_0     14               NaN               NaN              NaN   \n",
       "\n",
       "   IQR_acc_x_LWrist  IQR_acc_x_RAnkle  IQR_acc_x_RElbow  IQR_acc_x_RKnee  \\\n",
       "0               NaN               NaN               NaN              NaN   \n",
       "1               NaN               NaN               NaN              NaN   \n",
       "2               NaN               NaN               NaN              NaN   \n",
       "3               NaN               NaN               NaN              NaN   \n",
       "4               NaN               NaN               NaN              NaN   \n",
       "\n",
       "   IQR_acc_x_RWrist  ...  median_y_LAnkle  median_y_LElbow  median_y_LKnee  \\\n",
       "0               NaN  ...         1.626879         0.036969        1.222065   \n",
       "1               NaN  ...         1.625425         0.036544        1.220938   \n",
       "2               NaN  ...         1.623971         0.036118        1.219811   \n",
       "3               NaN  ...         1.622707         0.035772        1.218857   \n",
       "4               NaN  ...         1.621444         0.035426        1.217903   \n",
       "\n",
       "   median_y_LWrist  median_y_RAnkle  median_y_RElbow  median_y_RKnee  \\\n",
       "0         0.212976         1.471905        -0.047890        1.157515   \n",
       "1         0.211364         1.471938        -0.049638        1.156081   \n",
       "2         0.209752         1.471970        -0.051385        1.154647   \n",
       "3         0.207982         1.472125        -0.053796        1.153433   \n",
       "4         0.206211         1.472280        -0.056207        1.152219   \n",
       "\n",
       "   median_y_RWrist  lrCorr_x_Ankle  lrCorr_x_Wrist  \n",
       "0        -0.374298             NaN             NaN  \n",
       "1        -0.375767       -1.000000       -1.000000  \n",
       "2        -0.377236       -0.976615       -0.996211  \n",
       "3        -0.378959       -0.988410       -0.978462  \n",
       "4        -0.380681       -0.991569       -0.953311  \n",
       "\n",
       "[5 rows x 92 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/workspaces/gma_score_prediction_from_video/pose_estimates/CHOP_pose_estimates/xy_features/windows/139_1_0_features_windows_xy.csv')\n",
    "\n",
    "# Retain only columns with specified keywords\n",
    "keywords = ['wrist', 'ankle', 'elbow', 'knee']\n",
    "filtered_columns = [col for col in df.columns if any(keyword in col.lower() for keyword in keywords)]\n",
    "\n",
    "# Create a new DataFrame with only the filtered columns\n",
    "df_filtered = df[filtered_columns]\n",
    "\n",
    "# If you want to keep 'video' and 'frame' columns, include them explicitly\n",
    "df_filtered = df[['video', 'frame'] + filtered_columns]\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_filtered\n",
    "\n",
    "exclude_columns = ['video','frame']\n",
    "columns_to_plot = [col for col in df.columns if col not in exclude_columns]\n",
    "\n",
    "bp = 'elbow'\n",
    "columns = [col for col in columns_to_plot if bp in col.lower()]\n",
    "\n",
    "# Create scatterplots\n",
    "# Number of subplots\n",
    "n_cols = 4  # Number of columns in the grid\n",
    "n_rows = -(-len(ankle_columns) // n_cols)  # Ceiling division for the number of rows\n",
    "\n",
    "# Create a figure and subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 6 * n_rows), sharex=False, sharey=False)\n",
    "\n",
    "# Flatten axes for easy iteration (if there's only one row, make sure it's iterable)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each column in its subplot\n",
    "for i, column in enumerate(columns):\n",
    "    axes[i].scatter(df['frame'], df[column], alpha=0.5, label=column)\n",
    "    axes[i].set_title(f'{column}')\n",
    "    axes[i].set_xlabel('Frame')\n",
    "    axes[i].set_ylabel(column)\n",
    "    axes[i].grid(True)\n",
    "\n",
    "# Remove empty subplots if there are any\n",
    "for j in range(len(columns), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhWbYFEQ5L8X"
   },
   "outputs": [],
   "source": [
    "# Combine global features \n",
    "SAVE = True\n",
    "\n",
    "features_xy = pd.DataFrame()\n",
    "features_angle = pd.DataFrame()\n",
    "\n",
    "for csv in os.listdir(f'{pose_estimate_path}/xy_features/total'):\n",
    "    df = pd.read_csv(f'{pose_estimate_path}/xy_features/total/{csv}')\n",
    "    features_xy = pd.concat([features_xy, df], axis=0)\n",
    "\n",
    "for csv in os.listdir(f'{pose_estimate_path}/angle_features/total'):\n",
    "    df = pd.read_csv(f'{pose_estimate_path}/angle_features/total/{csv}')\n",
    "    features_angle = pd.concat([features_angle, df], axis=0)\n",
    "\n",
    "features = pd.merge(features_xy, features_angle, on='video', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{pose_estimate_path}/window_features', exist_ok=True)\n",
    "\n",
    "for csv in os.listdir(f'{pose_estimate_path}/xy_features/windows'):\n",
    "    video_number = csv.split('_')[0]\n",
    "    xy = pd.read_csv(f'{pose_estimate_path}/xy_features/windows/{csv}')   \n",
    "    ang = pd.read_csv(f'{pose_estimate_path}/xy_features/windows/{csv}'.replace('xy','angle'))\n",
    "\n",
    "    features = pd.merge(xy, ang, on=['video','frame'], how='inner').dropna()\n",
    "    features.to_csv(f'{pose_estimate_path}/window_features/{csv}'.replace('xy','all'), header=True, index=False)\n",
    "\n",
    "    del xy, ang, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fKR50_t6F3n"
   },
   "outputs": [],
   "source": [
    "# Split features based on video file naming convention\n",
    "\n",
    "if dataset == 'Youtube':\n",
    "    features['infant'] = features['video'].str.split('_').str.get(1).str[-3:]\n",
    "    features['session'] = features['video'].str.split('_').str.get(1).str[0]\n",
    "    features['age'] = 'month'\n",
    "\n",
    "elif dataset == 'Clinical':\n",
    "    features['infant'] = features['video'].str.split('_').str.get(0).str[-1]\n",
    "    features['session'] = features['video'].str.split('_').str.get(1).str[1]\n",
    "    features['age'] = 'month'\n",
    "\n",
    "elif dataset == 'gma_score_prediction':\n",
    "    features['infant'] = features['video']\n",
    "    features['session'] = 0\n",
    "    features['age'] = '3_4_Month'\n",
    "\n",
    "if SAVE:\n",
    "    features.to_csv(f'{pose_estimate_path}/features.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fw5_xTrfi9tA"
   },
   "outputs": [],
   "source": [
    "# Body parts and sides\n",
    "body_parts = [\"Knee\", \"Shoulder\", \"Hip\", \"Elbow\", \"Wrist\", \"Ankle\"]\n",
    "sides = [\"L\", \"R\"]\n",
    "\n",
    "# Function to split the feature string into \"feature\", \"part\", and \"side\"\n",
    "def split_feature(feature):\n",
    "    # Split the feature into components\n",
    "    parts = feature.split('_')\n",
    "    feature_name = \"_\".join(parts[:-1])  # Default to everything before the last part\n",
    "    part = parts[-1] if len(parts) > 1 else \"\"  # Default to the last part\n",
    "\n",
    "    side = \"\"\n",
    "\n",
    "    # Check if the last part has a side\n",
    "    for body_part in body_parts:\n",
    "        if body_part in part:  # Find body part\n",
    "            idx = part.index(body_part)\n",
    "            if idx > 0 and part[idx-1] in sides:  # Check for side prefix\n",
    "                side = part[idx-1]\n",
    "                part = part[idx:]  # Remove the side from the part\n",
    "            break  # Exit loop after finding the body part\n",
    "\n",
    "    # Adjust feature name to remove body part and side, if found\n",
    "    if part in feature_name:\n",
    "        feature_name = feature_name.replace(part, \"\").strip(\"_\")\n",
    "    if side in feature_name:\n",
    "        feature_name = feature_name.replace(side, \"\").strip(\"_\")\n",
    "\n",
    "    return pd.Series([feature_name, part, side])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "Td9C5LWE67Iv",
    "outputId": "732f3a60-77ef-4ccb-a7f9-921c9edc7863"
   },
   "outputs": [],
   "source": [
    "# Include data for the following, and restructure dataframe\n",
    "# Customize for your specific dataset\n",
    "\n",
    "id_vars = ['infant', 'age','session','video']\n",
    "melted = pd.melt(features, id_vars=id_vars, var_name=\"feature\", value_name=\"Value\")\n",
    "\n",
    "# Apply the function to the 'feature' column in your melted DataFrame\n",
    "melted[['feature', 'part', 'side']] = melted['feature'].apply(split_feature)\n",
    "melted = melted.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = melted.groupby(['infant', 'age', 'session', 'video', 'feature', 'part'])['Value'].mean().reset_index()\n",
    "mean.to_csv(f'{pose_estimate_path}/{dataset}_features_mean_by_side.csv', header=True, index=False)\n",
    "\n",
    "mean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLCqy4CzZtsZ"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "predict_gma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
