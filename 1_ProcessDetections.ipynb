{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ngtR6tOyIFM-",
    "outputId": "12e32194-6ae4-45c0-f4e5-3e1a3d8c66f8"
   },
   "outputs": [],
   "source": [
    "!pip install pandas==1.5.3\n",
    "\n",
    "import warnings\n",
    "import sys, os, cv2, glob, json, gc, shutil\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from moviepy.editor import VideoFileClip\n",
    "import skvideo.io\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import circstat as CS\n",
    "import scipy as sc\n",
    "import math, random\n",
    "\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from processing import *\n",
    "from kinematics import *\n",
    "from skeleton import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.makedirs('./data',exist_ok=True)\n",
    "os.makedirs('./utils',exist_ok=True)\n",
    "\n",
    "#download custom processing scripts if not already downloaded\n",
    "# !wget -P . https://raw.githubusercontent.com/quietscientist/gma_score_prediction_from_video/refs/heads/main/utils/kinematics.py\n",
    "# !wget -P . https://raw.githubusercontent.com/quietscientist/gma_score_prediction_from_video/refs/heads/main/utils/circstat.py\n",
    "# !wget -P . https://raw.githubusercontent.com/quietscientist/gma_score_prediction_from_video/refs/heads/main/utils/processing.py\n",
    "# !wget -P . https://raw.githubusercontent.com/quietscientist/gma_score_prediction_from_video/refs/heads/main/utils/skeleton.py\n",
    "\n",
    "#download example data or upload your own json annotations\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "# Download example raw data from figshare or specify path to your own json annotations |\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "#!wget -P . https://figshare.com/ndownloader/articles/25316500/versions/1\n",
    "# #unzip data into ./data folder and remove zip file\n",
    "# !unzip ./1 -d ./data\n",
    "# !rm ./1\n",
    "#uncomment install if running on google colab\n",
    "#%pip install scikit-video\n",
    "\n",
    "DEIDENTIFY = True #set to True to deidentify data, and exclude xy coordinates and pixel values from output\n",
    "OVERWRITE = True\n",
    "USE_CENTER_INSTANCE = False\n",
    "USE_BEST_INSTANCE = True\n",
    "\n",
    "dataset = 'gma_score_prediction'\n",
    "json_path = f'./data/Infant Pose Data/{dataset}/annotations'\n",
    "json_files = os.listdir(json_path)\n",
    "directory = f'./data'\n",
    "\n",
    "save_path = f'./pose_estimates/{dataset}_pose_estimates'\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "kp_mapping = {0:'Nose', 1:'Neck', 2:'RShoulder', 3:'RElbow', 4:'RWrist', 5:'LShoulder', 6:'LElbow',\n",
    "              7:'LWrist', 8:'RHip', 9:'RKnee', 10:'RAnkle', 11:'LHip',\n",
    "              12:'LKnee', 13:'LAnkle', 14:'REye', 15:'LEye', 16:'REar', 17:'LEar'}\n",
    "\n",
    "# Define the DataFrame columns as specified\n",
    "columns = ['video_number', 'video', 'bp', 'frame', 'x', 'y', 'c','fps', 'pixel_x', 'pixel_y', 'time', 'part_idx']\n",
    "data = []  # This will hold the data to be loaded into the DataFrame\n",
    "\n",
    "vid_info = pd.read_csv(f'./data/{dataset}_video_info.csv')\n",
    "\n",
    "# Smooth detections and compute features\n",
    "pose_estimate_path = f'./pose_estimates/{dataset}_pose_estimates'\n",
    "csv_path = f'{pose_estimate_path}/pose_estimates_{dataset}.csv'\n",
    "save_path = f'{pose_estimate_path}/pose_estimates_{dataset}_processed.csv'\n",
    "\n",
    "# List of subdirectories to create\n",
    "subdirs = [\n",
    "    \"\",\n",
    "    \"xdf\",\n",
    "    \"adf\",\n",
    "    \"xy_features\",\n",
    "    \"angle_features\",\n",
    "    \"xy_features/total\",\n",
    "    \"angle_features/total\",\n",
    "    \"xy_features/windows\",\n",
    "    \"angle_features/windows\",\n",
    "    \"smooth\",\n",
    "    \"anim\"\n",
    "]\n",
    "\n",
    "# Create necessary directories\n",
    "for subdir in subdirs:\n",
    "    os.makedirs(f'{pose_estimate_path}/{subdir}', exist_ok=True)\n",
    "\n",
    "print(pose_estimate_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ7Dz7hYfoAg",
    "outputId": "dd4ea572-f5c7-4660-b7d2-f07381427692"
   },
   "outputs": [],
   "source": [
    "# format files as pkl with openpose standard and bodypart labels\n",
    "\n",
    "def process_file(args):\n",
    "    \"\"\"Function to process a single file.\"\"\"\n",
    "    file_number, file, json_path, save_path, vid_info, kp_mapping = args\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(json_path, file)\n",
    "    fname = file.split('.')[0]\n",
    "    interim = []\n",
    "\n",
    "    if not OVERWRITE and os.path.exists(f'{save_path}/{fname}.pkl'):\n",
    "        return\n",
    "\n",
    "    # Open and load the JSON data\n",
    "    try: \n",
    "        with open(file_path, 'r') as f:\n",
    "            frames = json.load(f)\n",
    "            info = vid_info[vid_info['video'] == fname]\n",
    "            fps = vid_info['fps'].values[0]\n",
    "\n",
    "            pixel_x = vid_info['width'].values[0]\n",
    "            pixel_y = vid_info['height'].values[0]\n",
    "            \n",
    "            center_x = pixel_x / 2\n",
    "            center_y = pixel_y / 2\n",
    "            \n",
    "            # Iterate through each frame in the JSON file\n",
    "            for frame in frames:\n",
    "                frame_id = frame['frame_id']\n",
    "                if 'instances' in frame and len(frame['instances']) > 0:\n",
    "\n",
    "                    if USE_CENTER_INSTANCE:\n",
    "                        instance_id = get_center_instance(frame['instances'], center_x, center_y)\n",
    "                    elif USE_BEST_INSTANCE:\n",
    "                        instance_id = get_best_instance(frame['instances'])\n",
    "                    else:\n",
    "                        instance_id = 0\n",
    "\n",
    "                    keypoints = frame['instances'][instance_id]['keypoints']\n",
    "                    confidence = frame['instances'][instance_id]['keypoint_scores']\n",
    "                    keypoints, confidence = convert_coco_to_openpose(keypoints, confidence)\n",
    "\n",
    "                    # Iterate through each keypoint\n",
    "                    for part_idx, (x, y) in enumerate(keypoints):\n",
    "\n",
    "                        bp = kp_mapping[part_idx]\n",
    "                        fps = fps\n",
    "                        time = frame_id / fps\n",
    "                        c = confidence[part_idx]\n",
    "\n",
    "                        row = [file_number, fname, bp, frame_id, x, y, c, fps, pixel_x, pixel_y, time, part_idx]\n",
    "                        interim.append(row)\n",
    "\n",
    "        interim_df = pd.DataFrame(interim, columns=columns)\n",
    "        interim_df.to_csv(f'{save_path}/{fname}.csv', index=False)\n",
    "\n",
    "        del interim_df\n",
    "        return\n",
    "    \n",
    "    except Exception as e:\n",
    "        return\n",
    "    \n",
    "def process_annotations_multiprocess(json_files, json_path, save_path, vid_info, kp_mapping):\n",
    "    \"\"\"Run the annotation processing using multiprocessing.\"\"\"\n",
    "    args = [\n",
    "        (file_number, file, json_path, save_path, vid_info, kp_mapping)\n",
    "        for file_number, file in enumerate(json_files)\n",
    "    ]\n",
    "\n",
    "    # Set up a pool of workers\n",
    "    with Pool(processes=20) as pool:\n",
    "        pool.map(process_file, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_annotations_multiprocess(json_files, json_path, save_path, vid_info, kp_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gF9SA7KO1jbQ",
    "outputId": "0f87b278-d65f-4fba-9149-bd36844f6390"
   },
   "outputs": [],
   "source": [
    "# Ensure the save_path directory exists\n",
    "save_path = f'./pose_estimates/{dataset}_norm'\n",
    "\n",
    "if os.path.exists(f'{save_path}/pose_estimates_{dataset}.csv'):\n",
    "    os.remove(f'{save_path}/pose_estimates_{dataset}.csv')\n",
    "    print('Removed existing CSV file')\n",
    "\n",
    "for pklfile in tqdm(os.listdir(save_path)):\n",
    "    if not pklfile.endswith('.pkl'):\n",
    "        continue\n",
    "    else:\n",
    "        interim_df = pd.read_pickle(f'{save_path}/{pklfile}')\n",
    "        interim_df.to_csv(f'{save_path}/pose_estimates_{dataset}.csv', mode='a', header=False, index=False)\n",
    "\n",
    "    del interim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERK1DQ-92DsG"
   },
   "outputs": [],
   "source": [
    "csv_path = f'{save_path}/pose_estimates_{dataset}.csv'\n",
    "output_csv_path = f'{save_path}/pose_estimates_{dataset}_b.csv'\n",
    "chunksize = 1000  # Number of rows per chunk\n",
    "\n",
    "# Define the new headers\n",
    "new_headers = ['video_number', 'video', 'bp', 'frame', 'x', 'y', 'c', 'fps', 'pixel_x', 'pixel_y', 'time', 'part_idx']\n",
    "\n",
    "# Read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_path, chunksize=chunksize)\n",
    "\n",
    "# Process the first chunk\n",
    "first_chunk = next(chunk_iterator)\n",
    "first_chunk.columns = new_headers\n",
    "first_chunk.to_csv(output_csv_path, mode='w', index=False)\n",
    "\n",
    "# Process the rest of the chunks and append them to the new CSV file without headers\n",
    "for chunk in chunk_iterator:\n",
    "    chunk.columns = new_headers\n",
    "    chunk.to_csv(output_csv_path, mode='a', index=False, header=False)\n",
    "\n",
    "# rename the csv file\n",
    "os.rename(csv_path, f'{save_path}/pose_estimates_{dataset}_x.csv')\n",
    "os.rename(output_csv_path, csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "H5A-LkovEMav"
   },
   "outputs": [],
   "source": [
    "def process_dataframe(file):\n",
    "    df = pd.read_csv(os.path.join(pose_estimate_path, file))\n",
    "\n",
    "    if df.empty:\n",
    "        #print(\"DataFrame is empty, skipping processing.\")\n",
    "        return\n",
    "    # print(f\"Processing DataFrame for video_number: {df['video_number'].iloc[0]}\")\n",
    "    try:\n",
    "        if dataset == 'Youtube':\n",
    "            session = df['video'].unique()[0].split('_')[1][0]\n",
    "            infant = df['video'].unique()[0].split('_')[1][3:]\n",
    "            age = '3Month'\n",
    "        elif dataset == 'Clinical':\n",
    "            # split based on naming convention\n",
    "            session = df['video'].unique()[0].split('_')[1][1]\n",
    "            infant = df['video'].unique()[0].split('_')[0][-1]\n",
    "            age = '3Month'\n",
    "        elif dataset == 'gma_score_prediction': \n",
    "            session = 0\n",
    "            infant = df['video'].unique()[0]\n",
    "            age = '3Month'\n",
    "        elif dataset == 'CHOP': \n",
    "            session = df['video'].unique()[0].split('_')[1]\n",
    "            infant = df['video'].unique()[0].split('_')[0]\n",
    "            age = df['video'].unique()[0].split('_')[2]\n",
    "        \n",
    "        # print(f'infant: {infant} {session} {age}')\n",
    "\n",
    "\n",
    "        median_window = 1\n",
    "        mean_window = 1\n",
    "        delta_window = 0.25  # Smoothing applied to delta_x, velocity, acceleration\n",
    "\n",
    "        df['x'] = pd.to_numeric(df['x'])\n",
    "        df['y'] = pd.to_numeric(df['y'])\n",
    "\n",
    "        #filter low confidence detections\n",
    "        #df = df[df['c'] > 0.5]\n",
    "\n",
    "        # Interpolate\n",
    "        df = df.groupby(['video', 'bp']).apply(interpolate_df).reset_index(drop=True)\n",
    "\n",
    "        # Median and mean filter\n",
    "        median_window = 0.5\n",
    "        mean_window = 0.5\n",
    "        df = df.groupby(['video', 'bp']).apply(lambda x: smooth(x, 'y', median_window, mean_window)).reset_index(drop=True)\n",
    "        df = df.groupby(['video', 'bp']).apply(lambda x: smooth(x, 'x', median_window, mean_window)).reset_index(drop=True)\n",
    "        \n",
    "        df = normalise_skeletons(df) \n",
    "        df.to_csv(f'{pose_estimate_path}/smooth/{infant}_{session}_{age}_smooth_norm.csv')\n",
    "    \n",
    "    except:\n",
    "        f'could not process video {df[\"video\"].unique()[0]}'\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Rotate and normalise by reference\n",
    "        xdf = get_dynamics_xy(df, delta_window)    \n",
    "        xdf.to_csv(f'{pose_estimate_path}/xdf/{infant}_{session}_{age}_smooth_norm_xy.csv')\n",
    "\n",
    "        adf = get_joint_angles(df)\n",
    "        adf = get_dynamics_angle(adf, delta_window)\n",
    "        adf.to_csv(f'{pose_estimate_path}/adf/{infant}_{session}_{age}_smooth_norm_ang.csv')\n",
    "\n",
    "    except KeyError as e:\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_estimate_files = os.listdir(pose_estimate_path)\n",
    "pose_estimate_files = [file for file in pose_estimate_files if file.endswith('.csv')]\n",
    "\n",
    "# pose_estimate_files = random.sample(pose_estimate_files, 10)\n",
    "\n",
    "print(f'Processing {len(pose_estimate_files)} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(processes=25) as pool:\n",
    "    pool.map(process_dataframe, pose_estimate_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0],\n",
    "          [0, 255, 0], \\\n",
    "          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255],\n",
    "          [85, 0, 255], \\\n",
    "          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85],[255, 0, 0]]\n",
    "\n",
    "limbSeq = [[2, 3], [2, 6], [3, 4], [4, 5], [6, 7], [7, 8], [2, 9], [9, 10], \\\n",
    "           [10, 11], [2, 12], [12, 13], [13, 14], [2, 1], [1, 15], [15, 17], \\\n",
    "           [1, 16], [16, 18]] #[3, 17], [6, 18]]\n",
    "\n",
    "def plot_skel(df, ax, xvar, yvar):\n",
    "    alpha = 0.3\n",
    "\n",
    "    for i, limb in enumerate(limbSeq):\n",
    "        l1 = limb[0] - 1\n",
    "        l2 = limb[1] - 1\n",
    "        df_l1 = df[df.part_idx == l1]\n",
    "        df_l2 = df[df.part_idx == l2]\n",
    "        if not df_l1.empty and not df_l2.empty:\n",
    "            ax.plot(\n",
    "                [df_l1[xvar].iloc[0], df_l2[xvar].iloc[0]],\n",
    "                [df_l1[yvar].iloc[0], df_l2[yvar].iloc[0]],\n",
    "                linewidth=5,\n",
    "                color=[j / 255 for j in colors[i]],\n",
    "                alpha=alpha,\n",
    "            )\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        ax.plot(\n",
    "            df.iloc[i][xvar],\n",
    "            df.iloc[i][yvar],\n",
    "            'o',\n",
    "            markersize=10,\n",
    "            color=[j / 255 for j in colors[i]],\n",
    "            alpha=alpha,\n",
    "        )\n",
    "\n",
    "\n",
    "def animate_coordinates_with_skeleton(file):\n",
    "    \n",
    "    file = f'{pose_estimate_path}/smooth/{file}'\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    fname = os.path.basename(file)\n",
    "\n",
    "    fps = df.fps[0]\n",
    "    framen = df.frame.max()\n",
    "    frame_interval = 5\n",
    "    dpi = 80\n",
    "\n",
    "    output_gif = f'{pose_estimate_path}/anim/{os.path.splitext(fname)[0]}.gif'\n",
    "    print(output_gif)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6), dpi=dpi)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "    ax.invert_yaxis()\n",
    "    ax.axis('off')\n",
    "\n",
    "    def update(frame_idx):\n",
    "        ax.clear()\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        ax.set_xlim(-3, 3)\n",
    "        ax.set_ylim(-3, 3)\n",
    "        ax.invert_yaxis()\n",
    "        ax.axis('off')\n",
    "        plot_skel(df[df.frame == frame_idx], ax, 'x', 'y')\n",
    "\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig,\n",
    "        update,\n",
    "        frames=range(0, framen, frame_interval),\n",
    "        interval=int(1 / fps * 1000 * frame_interval),\n",
    "        repeat_delay=1000\n",
    "    )\n",
    "\n",
    "    ani.save(output_gif, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "\n",
    "    del ani, df, fig, ax\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_files = os.listdir(f'{pose_estimate_path}/smooth')\n",
    "smooth_files = [file for file in smooth_files if file.endswith('.csv')]\n",
    "\n",
    "output_names = os.listdir(f'{pose_estimate_path}/anim')\n",
    "output_names = [file for file in output_names if file.endswith('.gif')]\n",
    "\n",
    "match = []\n",
    "for name in output_names:\n",
    "    parts = name.split('.')[0]\n",
    "    match.append(f'{parts}.csv')\n",
    "\n",
    "smooth_files = [file for file in smooth_files if file not in match]\n",
    "#smooth_files = random.sample(smooth_files, 5)\n",
    "\n",
    "print(len(smooth_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(processes=25) as pool:\n",
    "    pool.map(animate_coordinates_with_skeleton, smooth_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_files = os.listdir(f'{pose_estimate_path}/smooth')\n",
    "smooth_files = [file for file in smooth_files if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributions to check for outliers\n",
    "def get_number_of_frames(file):\n",
    "    # Read only necessary columns to reduce memory usage\n",
    "    df = pd.read_csv(file, usecols=['video', 'frame'])\n",
    "    # Get just the filename without path and extension\n",
    "    filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    # Split the filename on underscores\n",
    "    # Extract unique video info (assumes consistent naming convention)\n",
    "    video_info = filename.split('_')\n",
    "    infant = video_info[0]\n",
    "    session = video_info[1]\n",
    "    age = video_info[2]\n",
    "    \n",
    "    # Get the maximum frame number\n",
    "    n = df['frame'].max()\n",
    "    \n",
    "    # Return the result as a dictionary\n",
    "    return {\n",
    "        'infant': infant,\n",
    "        'session': session,\n",
    "        'age': age,\n",
    "        'n_frames': n\n",
    "    }\n",
    "\n",
    "def gather_frame_counts(files):\n",
    "    for file in files:\n",
    "        file_path = f'{pose_estimate_path}/smooth/{file}'\n",
    "        yield get_number_of_frames(file_path)  # Use a generator for memory efficiency\n",
    "\n",
    "\n",
    "smooth_files = os.listdir(f'{pose_estimate_path}/smooth')\n",
    "smooth_files = [file for file in smooth_files if file.endswith('.csv')]\n",
    "\n",
    "frame_counts = list(gather_frame_counts(smooth_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{pose_estimate_path}/distributions', exist_ok=True)\n",
    "\n",
    "frame_counts_df = pd.DataFrame(frame_counts)\n",
    "frame_counts_df.to_csv(f'{pose_estimate_path}/distributions/frame_counts.csv', index=False)\n",
    "#frame_counts_df = pd.read_csv(f'{pose_estimate_path}/distributions/frame_counts.csv')\n",
    "\n",
    "# Define custom colors for each age\n",
    "age_colors = {0: '#56B4E9', 1: '#E69F00'}  # Blue and Orange from the CUD palette\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Group by 'age' and overlay histograms with specified colors\n",
    "for age, group in frame_counts_df.groupby('age'):\n",
    "    color = age_colors.get(age, 'gray')  # Default to 'gray' if age is not in age_colors\n",
    "    plt.hist(group['n_frames'], bins=100, alpha=0.5, label=f'Age: {age}', color=color)\n",
    "\n",
    "plt.title('Frame Count Distribution by Age')\n",
    "plt.xlabel('Number of Frames')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f'{pose_estimate_path}/distributions/frame_counts.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0ik4gOyCDVZ",
    "outputId": "f16fdcc3-5798-47cc-f78c-282d79d7e7ae"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "chunksize = 100000\n",
    "buffer = pd.DataFrame()\n",
    "\n",
    "# Read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_path, chunksize=chunksize)\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "  unique_videos = chunk['video_number'].unique()\n",
    "\n",
    "  for video_number in unique_videos:\n",
    "      video_chunk = chunk[chunk['video_number'] == video_number]\n",
    "\n",
    "      if not buffer.empty:\n",
    "        if buffer['video_number'].iloc[0] == video_number:\n",
    "\n",
    "            buffer = pd.concat([buffer, video_chunk], ignore_index=True)\n",
    "            if video_number not in chunk['video_number'].values:\n",
    "                process_dataframe(buffer, pose_estimate_path)\n",
    "                buffer = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            process_dataframe(buffer, pose_estimate_path)\n",
    "            buffer = video_chunk\n",
    "      else:\n",
    "          buffer = video_chunk\n",
    "  clear_output(wait=True)\n",
    "  chunk = chunk[~chunk['video_number'].isin(unique_videos)]\n",
    "\n",
    "# # Process any remaining rows in the buffer\n",
    "if not buffer.empty:\n",
    "    print(\"Processing remaining rows in the buffer...\")\n",
    "    process_dataframe(buffer, pose_estimate_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AenvmNmB3cc5",
    "outputId": "7925b160-4c5f-45f2-fe96-c02775cf3106"
   },
   "outputs": [],
   "source": [
    "#compute xy features\n",
    "\n",
    "def process_xdf_file(file): \n",
    "        \n",
    "        xdf = pd.read_csv(os.path.join(f'{pose_estimate_path}/xdf', file))\n",
    "\n",
    "        bps = ['LAnkle', 'RAnkle', 'LWrist', 'RWrist']\n",
    "        filtered_xdf = xdf[np.isin(xdf.bp, bps)]\n",
    "        video_number = xdf.video.unique()[0]\n",
    "        \n",
    "        # Compute window xy features: \n",
    "        mean_type = 'windows'\n",
    "        feature_xy = xdf.groupby(['bp', 'video']).apply(lambda group: rolling_xy_features(group, window_size=60)).reset_index(drop=True)\n",
    "        feature_xy = pd.pivot_table(feature_xy, index=['video','frame'], columns=['bp'])\n",
    "           \n",
    "        l0 = feature_xy.columns.get_level_values(1)\n",
    "        l1 = feature_xy.columns.get_level_values(0)\n",
    "\n",
    "        cols = [l1[i]+'_'+l0[i] for i in range(len(l1))]\n",
    "        feature_xy.columns = cols\n",
    "        feature_xy = feature_xy.reset_index()\n",
    "\n",
    "        # - measure of symmetry (left-right cross correlation)\n",
    "        xdf['dist'] = np.sqrt(xdf['x']**2+xdf['y']**2)\n",
    "        corr_joint = xdf[np.isin(xdf.bp, bps)].groupby(['video', 'part']).apply(lambda x:rolling_corr_lr(x,var='dist')).reset_index()\n",
    "        corr_joint['part'] = 'lrCorr_x_'+corr_joint['part']\n",
    "        corr_joint.drop(columns=['level_2','R','L'],inplace=True)\n",
    "\n",
    "        corr_joint.columns = ['video', 'feature', 'frame', 'Value']\n",
    "        corr_joint = pd.pivot_table(corr_joint, index=['video', 'frame'], columns=['feature'])\n",
    "        l1 = corr_joint.columns.get_level_values(1)\n",
    "        corr_joint.columns = l1\n",
    "        corr_joint = corr_joint.reset_index()\n",
    "        feature_xy = pd.merge(feature_xy, corr_joint, on=['video','frame'], how='outer')\n",
    "\n",
    "        feature_xy.to_csv(f'{pose_estimate_path}/xy_features/{mean_type}/{video_number}_features_{mean_type}_xy.csv', header=True, index=False)\n",
    "    \n",
    "        #compute total xy features (average by video)\n",
    "        mean_type = 'total'\n",
    "\n",
    "        feature_xy = filtered_xdf.groupby(['bp','video']).apply(xy_features).reset_index(drop=True)\n",
    "        feature_xy = pd.pivot_table(feature_xy, index='video', columns=['bp'])\n",
    "        l0 = feature_xy.columns.get_level_values(1)\n",
    "        l1 = feature_xy.columns.get_level_values(0)\n",
    "        cols = [l1[i]+'_'+l0[i] for i in range(len(l1))]\n",
    "        feature_xy.columns = cols\n",
    "        feature_xy = feature_xy.reset_index()\n",
    "\n",
    "        # - measure of symmetry (left-right cross correlation)\n",
    "\n",
    "        xdf['dist'] = np.sqrt(xdf['x']**2+xdf['y']**2)\n",
    "        corr_joint = xdf.groupby(['video', 'part']).apply(lambda x:corr_lr(x,'dist')).reset_index()\n",
    "        corr_joint['part'] = 'lrCorr_x_'+corr_joint['part']\n",
    "        corr_joint.columns = ['video', 'feature', 'Value']\n",
    "        corr_joint = pd.pivot_table(corr_joint, index='video', columns=['feature'])\n",
    "        l1 = corr_joint.columns.get_level_values(1)\n",
    "        corr_joint.columns = l1\n",
    "        corr_joint = corr_joint.reset_index()\n",
    "        feature_xy = pd.merge(feature_xy, corr_joint, on='video', how='outer')\n",
    "        \n",
    "        feature_xy.to_csv(f'{pose_estimate_path}/xy_features/{mean_type}/{video_number}_features_{mean_type}_xy.csv', header=True, index=False)\n",
    "\n",
    "        return\n",
    "\n",
    "# Compute angular features\n",
    "\n",
    "def process_adf_file(file): \n",
    "\n",
    "        adf = pd.read_csv(os.path.join(f'{pose_estimate_path}/adf', file))\n",
    "        \n",
    "        video_number = adf.video.unique()[0]\n",
    "\n",
    "        # Compute window angle features: \n",
    "        mean_type = 'windows'\n",
    "        window_size = 2*int(adf['fps'].iloc[0]) # 2 seconds\n",
    "\n",
    "        # Compute window angle features: \n",
    "        feature_angle = adf.groupby(['bp','video']).apply(rolling_angle_features, window_size=window_size).reset_index(drop=True)\n",
    "        feature_angle = pd.pivot_table(feature_angle, index=['video','frame'], columns=['bp'])\n",
    "        l0 = feature_angle.columns.get_level_values(1)\n",
    "        l1 = feature_angle.columns.get_level_values(0)\n",
    "        cols = [l1[i]+'_'+l0[i] for i in range(len(l1))]\n",
    "        feature_angle.columns = cols\n",
    "        feature_angle =feature_angle.reset_index()\n",
    "\n",
    "        # - measure of symmetry (left-right cross correlation)\n",
    "        corr_joint = adf.groupby(['video', 'part']).apply(rolling_corr_lr, window_size=window_size, min_periods=1, var='angle')\n",
    "        corr_joint.reset_index(inplace=True)\n",
    "        corr_joint.drop(columns=['level_2','R','L'],inplace=True)\n",
    "        corr_joint['part'] = 'lrCorr_angle_'+corr_joint['part']\n",
    "        corr_joint.columns = ['video', 'feature', 'frame','Value']\n",
    "        corr_joint = pd.pivot_table(corr_joint, index=['video','frame'], columns=['feature'])\n",
    "        l1 = corr_joint.columns.get_level_values(1)\n",
    "        corr_joint.columns = l1\n",
    "        corr_joint = corr_joint.reset_index()\n",
    "\n",
    "        feature_angle = pd.merge(feature_angle,corr_joint, on=['video','frame'], how='outer')\n",
    "        feature_angle.to_csv(f'{pose_estimate_path}/angle_features/{mean_type}/{video_number}_features_{mean_type}_angle.csv', header=True, index=False)\n",
    "\n",
    "        #compute total angle features (average by video)\n",
    "        mean_type = 'total'\n",
    "        feature_angle = adf.groupby(['bp','video']).apply(angle_features).reset_index(drop=True)\n",
    "        feature_angle = pd.pivot_table(feature_angle, index='video', columns=['bp'])        \n",
    "        \n",
    "        l0 = feature_angle.columns.get_level_values(1)\n",
    "        l1 = feature_angle.columns.get_level_values(0)\n",
    "        cols = [l1[i]+'_'+l0[i] for i in range(len(l1))]\n",
    "        feature_angle.columns = cols\n",
    "        feature_angle =feature_angle.reset_index()\n",
    "\n",
    "        # - measure of symmetry (left-right cross correlation)\n",
    "        corr_joint = adf.groupby(['video', 'part']).apply(lambda x:corr_lr(x,'angle')).reset_index()\n",
    "        corr_joint['part'] = 'lrCorr_angle_'+corr_joint['part']\n",
    "        corr_joint.columns = ['video', 'feature', 'Value']\n",
    "        corr_joint = pd.pivot_table(corr_joint, index='video', columns=['feature'])\n",
    "        l1 = corr_joint.columns.get_level_values(1)\n",
    "        corr_joint.columns = l1\n",
    "        corr_joint = corr_joint.reset_index()\n",
    "        feature_angle = pd.merge(feature_angle,corr_joint, on='video', how='outer')\n",
    "\n",
    "        feature_angle.to_csv(f'{pose_estimate_path}/angle_features/{mean_type}/{video_number}_features_{mean_type}_angle.csv', header=True, index=False)\n",
    "        \n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf_paths = os.listdir(f'{pose_estimate_path}/xdf')  # List of file paths\n",
    "xdf_paths = [file for file in xdf_paths if file.endswith('.csv')]\n",
    "\n",
    "#xdf_paths = random.sample(xdf_paths, 5)\n",
    "\n",
    "adf_paths = os.listdir(f'{pose_estimate_path}/adf')  # List of file paths\n",
    "adf_paths = [file for file in adf_paths if file.endswith('.csv')]\n",
    "\n",
    "#adf_paths = random.sample(adf_paths, 5)\n",
    "print(len(xdf_paths), len(adf_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(processes=24) as pool:\n",
    "    pool.map(process_xdf_file, xdf_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with Pool(processes=24) as pool:\n",
    "    pool.map(process_adf_file, adf_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/workspaces/gma_score_prediction_from_video/pose_estimates/gma_score_prediction_pose_estimates/xy_features/total/1000_features_total_xy.csv')\n",
    "\n",
    "# Retain only columns with specified keywords\n",
    "keywords = ['wrist', 'ankle', 'elbow', 'knee']\n",
    "filtered_columns = [col for col in df.columns if any(keyword in col.lower() for keyword in keywords)]\n",
    "\n",
    "# Create a new DataFrame with only the filtered columns\n",
    "df_filtered = df[filtered_columns]\n",
    "\n",
    "# If you want to keep 'video' and 'frame' columns, include them explicitly\n",
    "\n",
    "df_filtered = df[['video'] + filtered_columns]\n",
    "#df_filtered = df[['video'] + filtered_columns]\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_filtered\n",
    "\n",
    "exclude_columns = ['video','frame']\n",
    "columns_to_plot = [col for col in df.columns if col not in exclude_columns]\n",
    "\n",
    "bp = 'ankle'\n",
    "columns = [col for col in columns_to_plot if bp in col.lower()]\n",
    "\n",
    "# Create scatterplots\n",
    "# Number of subplots\n",
    "n_cols = 4  # Number of columns in the grid\n",
    "n_rows = -(-len(columns) // n_cols)  # Ceiling division for the number of rows\n",
    "\n",
    "# Create a figure and subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 6 * n_rows), sharex=False, sharey=False)\n",
    "\n",
    "# Flatten axes for easy iteration (if there's only one row, make sure it's iterable)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each column in its subplot\n",
    "for i, column in enumerate(columns):\n",
    "    axes[i].scatter(df['frame'], df[column], alpha=0.5, label=column)\n",
    "    axes[i].set_title(f'{column}')\n",
    "    axes[i].set_xlabel('Frame')\n",
    "    axes[i].set_ylabel(column)\n",
    "    axes[i].grid(True)\n",
    "\n",
    "# Remove empty subplots if there are any\n",
    "for j in range(len(columns), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_total_files = os.listdir(f'{pose_estimate_path}/angle_features/total')\n",
    "df = pd.read_csv(f'{pose_estimate_path}/angle_features/total/{angle_total_files[0]}')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_total_files = os.listdir(f'{pose_estimate_path}/angle_features/windows')\n",
    "df = pd.read_csv(f'{pose_estimate_path}/angle_features/windows/{angle_total_files[0]}')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "VhWbYFEQ5L8X"
   },
   "outputs": [],
   "source": [
    "# Combine global features \n",
    "SAVE = True\n",
    "keywords = ['wrist', 'ankle', 'elbow', 'knee']\n",
    "\n",
    "features_xy = pd.DataFrame()\n",
    "features_angle = pd.DataFrame()\n",
    "\n",
    "xy_total_files = os.listdir(f'{pose_estimate_path}/xy_features/total')\n",
    "for csv in os.listdir(f'{pose_estimate_path}/xy_features/total'):\n",
    "    df = pd.read_csv(f'{pose_estimate_path}/xy_features/total/{csv}')\n",
    "    features_xy = pd.concat([features_xy, df], axis=0)\n",
    "\n",
    "for csv in os.listdir(f'{pose_estimate_path}/angle_features/total'):\n",
    "    df = pd.read_csv(f'{pose_estimate_path}/angle_features/total/{csv}')\n",
    "    features_angle = pd.concat([features_angle, df], axis=0)\n",
    "\n",
    "features = pd.merge(features_xy, features_angle, on='video', how='inner')\n",
    "filtered_columns = [col for col in df.columns if any(keyword in col.lower() for keyword in keywords)]\n",
    "features = features[['video'] + filtered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.merge(features_xy, features_angle, on='video', how='inner')\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns = ['video']\n",
    "columns_to_plot = [col for col in features.columns if col not in exclude_columns]\n",
    "\n",
    "bp = 'ankle'\n",
    "columns = [col for col in columns_to_plot if bp in col.lower()]\n",
    "\n",
    "# Create scatterplots\n",
    "# Number of subplots\n",
    "n_cols = 4  # Number of columns in the grid\n",
    "n_rows = -(-len(columns) // n_cols)  # Ceiling division for the number of rows\n",
    "\n",
    "# Create a figure and subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 6 * n_rows), sharex=False, sharey=False)\n",
    "\n",
    "# Flatten axes for easy iteration (if there's only one row, make sure it's iterable)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each column in its subplot\n",
    "for i, column in enumerate(columns):\n",
    "    axes[i].scatter(features.index, features[column], alpha=0.5, label=column)\n",
    "    axes[i].set_title(f'{column}')\n",
    "    axes[i].set_xlabel('index')\n",
    "    axes[i].set_ylabel(column)\n",
    "    axes[i].grid(True)\n",
    "\n",
    "# Remove empty subplots if there are any\n",
    "for j in range(len(columns), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{pose_estimate_path}/window_features', exist_ok=True)\n",
    "\n",
    "for csv in os.listdir(f'{pose_estimate_path}/xy_features/windows'):\n",
    "    video_number = csv.split('_')[0]\n",
    "    xy = pd.read_csv(f'{pose_estimate_path}/xy_features/windows/{csv}')   \n",
    "    ang = pd.read_csv(f'{pose_estimate_path}/xy_features/windows/{csv}'.replace('xy','angle'))\n",
    "\n",
    "    features = pd.merge(xy, ang, on=['video','frame'], how='inner').dropna()\n",
    "    features.to_csv(f'{pose_estimate_path}/window_features/{csv}'.replace('xy','all'), header=True, index=False)\n",
    "\n",
    "    del xy, ang, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "6fKR50_t6F3n"
   },
   "outputs": [],
   "source": [
    "# Split features based on video file naming convention\n",
    "if dataset == 'Youtube':\n",
    "    features['infant'] = features['video'].str.split('_').str.get(1).str[-3:]\n",
    "    features['session'] = features['video'].str.split('_').str.get(1).str[0]\n",
    "    features['age'] = 'month'\n",
    "\n",
    "elif dataset == 'Clinical':\n",
    "    features['infant'] = features['video'].str.split('_').str.get(0).str[-1]\n",
    "    features['session'] = features['video'].str.split('_').str.get(1).str[1]\n",
    "    features['age'] = 'month'\n",
    "\n",
    "elif dataset == 'gma_score_prediction':\n",
    "    features['infant'] = features['video']\n",
    "    features['session'] = 0\n",
    "    features['age'] = '3_4_Month'\n",
    "\n",
    "if SAVE:\n",
    "    features.to_csv(f'{pose_estimate_path}/features.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "Fw5_xTrfi9tA"
   },
   "outputs": [],
   "source": [
    "# Body parts and sides for features avearaged across whole video \n",
    "features = pd.read_csv('pose_estimates/gma_score_prediction_pose_estimates/features.csv')\n",
    "\n",
    "body_parts = [\"Knee\", \"Elbow\", \"Wrist\", \"Ankle\",\"Hip\",\"Shoulder\"]\n",
    "final_feature_bp = ['Knee','Elbow','Wrist','Ankle']\n",
    "\n",
    "sides = [\"L\", \"R\"]\n",
    "\n",
    "# Function to split the feature string into \"feature\", \"part\", and \"side\"\n",
    "def split_feature(feature):\n",
    "    # Split the feature into components\n",
    "    parts = feature.split('_')\n",
    "    feature_name = \"_\".join(parts[:-1])  # Default to everything before the last part\n",
    "    part = parts[-1] if len(parts) > 1 else \"\"  # Default to the last part\n",
    "    side = \"\"\n",
    "\n",
    "    # Check if the last part has a side\n",
    "    for body_part in body_parts:\n",
    "        if body_part in part:  # Find body part\n",
    "            idx = part.index(body_part)\n",
    "            if idx > 0 and part[idx-1] in sides:  # Check for side prefix\n",
    "                side = part[idx-1]\n",
    "                part = part[idx:]  # Remove the side from the part            print(parts, idx)\n",
    "            break  # Exit loop after finding the body part\n",
    "\n",
    "    # # Adjust feature name to remove body part and side, if found\n",
    "    if part in feature_name:\n",
    "        feature_name = feature_name.replace(part, \"\").strip(\"_\")\n",
    "    if side + part in feature_name:  # Only remove the complete side+part combination\n",
    "        feature_name = feature_name.replace(side + part, \"\").strip(\"_\")\n",
    "    # print(feature_name)\n",
    "    return pd.Series([feature_name, part, side])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Include data for the following, and restructure dataframe\n",
    "# Customize for your specific dataset\n",
    "\n",
    "id_vars = ['infant', 'age','session','video']\n",
    "melted = pd.melt(features, id_vars=id_vars, var_name=\"feature\", value_name=\"Value\")\n",
    "\n",
    "# Apply the function to the 'feature' column in your melted DataFrame\n",
    "melted[['feature', 'part', 'side']] = melted['feature'].apply(split_feature)\n",
    "melted = melted.dropna()\n",
    "\n",
    "mean = melted.groupby(['infant', 'age', 'session', 'video', 'feature', 'part'])['Value'].mean().reset_index().drop(columns=['video'])\n",
    "#Drop all rows with part not in body_parts (case insensitive)\n",
    "# mean = mean[mean['part'].isin(final_feature_bp)]\n",
    "mean.to_csv(f'{pose_estimate_path}/features_mean_by_side.csv', header=True, index=False)\n",
    "\n",
    "\n",
    "mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Body parts and sides for features averaged across windows\n",
    "\n",
    "feature_files = os.listdir(f'{pose_estimate_path}/window_features')\n",
    "os.makedirs(f'{pose_estimate_path}/features_window_mean_by_side', exist_ok=True)\n",
    "\n",
    "\n",
    "features_windows_mean_by_side = pd.DataFrame()\n",
    "id_vars = ['video','frame']\n",
    "\n",
    "def process_feature_file(file):\n",
    "    # Read each feature file\n",
    "    features_current = pd.read_csv(f'{pose_estimate_path}/window_features/{file}')\n",
    "    \n",
    "    # Melt the current features dataframe\n",
    "    melted = pd.melt(features_current, id_vars=id_vars, var_name=\"feature\", value_name=\"Value\")\n",
    "\n",
    "    # Split feature column into components\n",
    "    melted[['feature', 'part', 'side']] = melted['feature'].apply(split_feature)\n",
    "    melted = melted.dropna()\n",
    "\n",
    "    # Calculate means for current file\n",
    "    mean = melted.groupby(['video', 'frame','feature', 'part'])['Value'].mean().reset_index()\n",
    "    mean = mean[mean['part'].isin(final_feature_bp)]\n",
    "    \n",
    "    # write to csv\n",
    "    mean.to_csv(f'{pose_estimate_path}/features_window_mean_by_side/{file}', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which files are missing\n",
    "missing_files = [file for file in feature_files if not os.path.exists(f'{pose_estimate_path}/features_window_mean_by_side/{file}')]\n",
    "print(len(missing_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with Pool(processes=10) as pool:\n",
    "    pool.map(process_feature_file, missing_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_part_combos = mean[['feature', 'part']].drop_duplicates()\n",
    "\n",
    "# Create subplot grid\n",
    "n_combos = len(feature_part_combos)\n",
    "n_cols = 3\n",
    "n_rows = (n_combos + n_cols - 1) // n_cols\n",
    "plt.figure(figsize=(20, 5*n_rows))\n",
    "\n",
    "# Plot distribution for each feature/part combination\n",
    "for idx, (_, combo) in enumerate(feature_part_combos.iterrows()):\n",
    "    feature = combo['feature']\n",
    "    part = combo['part']\n",
    "    \n",
    "    plt.subplot(n_rows, n_cols, idx+1)\n",
    "    \n",
    "\n",
    "    # Plot histograms with colorblind friendly colors\n",
    "    \n",
    "    subset = mean[(mean['feature'] == feature) & (mean['part'] == part)]\n",
    "    plt.hist(subset['Value'], bins=30, alpha=0.6, label='All Frames', color='#0077BB')  # Blue\n",
    "    \n",
    "    plt.title(f'{feature} - {part}')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each feature plot the distribution of the values across all videos (where each video is a different file)\n",
    "\n",
    "# get all files in the features_window_mean_by_side folder\n",
    "feature_files = os.listdir(f'{pose_estimate_path}/features_window_mean_by_side')\n",
    "file = pd.read_csv(f'{pose_estimate_path}/features_window_mean_by_side/{feature_files[0]}')\n",
    "\n",
    "feature_part_combos = file[['feature', 'part']].drop_duplicates()\n",
    "\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first feature/part combo\n",
    "all_files = os.listdir(f'{pose_estimate_path}/features_window_mean_by_side')\n",
    "output_dir = f'{pose_estimate_path}/window_mean_by_feature'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for idx, combo in feature_part_combos.iterrows():\n",
    "    subset = pd.DataFrame()\n",
    "    feature_name = f'{combo[\"feature\"]}_{combo[\"part\"]}'\n",
    "\n",
    "    feature = combo['feature']\n",
    "    part = combo['part']\n",
    "\n",
    "    # Get subset for first combo\n",
    "    for file in all_files:\n",
    "        file = pd.read_csv(f'{pose_estimate_path}/features_window_mean_by_side/{file}')\n",
    "        subset = pd.concat([subset, file[(file['feature'] == feature) & (file['part'] == part)]])\n",
    "\n",
    "    subset.to_csv(f'{output_dir}/{feature_name}.csv', header=True, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make one file per feature, with all the values for that feature across all videos\n",
    "for file in feature_files:\n",
    "    df = pd.read_csv(f'{pose_estimate_path}/features_window_mean_by_side/{file}')\n",
    "    feature = df['feature'].unique()[0]\n",
    "\n",
    "    df.to_csv(f'{pose_estimate_path}/features_window_mean_by_side/{feature}.csv', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the final features using the following dictionary\n",
    "\n",
    "feature_to_file_mapping = {\n",
    "\n",
    "    \"IQR ankle pos x\": \"IQR_x_Ankle\",\n",
    "    \"Med ankle vel x\": \"median_vel_x_Ankle\",\n",
    "    \"IQR ankle vel x\": \"IQR_vel_x_Ankle\",\n",
    "    \"Entropy ankle pos\": \"mean_ent_Ankle\",\n",
    "    \"IQR ankle accel x\": \"IQR_acc_x_Ankle\",\n",
    "    \"Med ankle vel y\": \"median_vel_y_Ankle\",\n",
    "    \"IQR ankle vel y\": \"IQR_vel_y_Ankle\",\n",
    "    \"Med knee angle vel\": \"median_vel_Knee\",\n",
    "    \"IQR knee angle vel\": \"IQR_vel_Knee\",\n",
    "    \"Stdev knee angle\": \"stdev_angle_Knee\",\n",
    "    \"Med wrist pos y\": \"median_y_Wrist\",\n",
    "    \"IQR ankle pos y\": \"IQR_y_Ankle\",\n",
    "    \"Entropy knee angle\": \"entropy_angle_Knee\",\n",
    "    \"IQR knee angle accel\": \"IQR_acc_Knee\",\n",
    "    \"Mean elbow angle\": \"mean_angle_Elbow\",\n",
    "    \"IQR ankle accel y\": \"IQR_acc_y_Ankle\",\n",
    "    \"Med ankle pos x\": \"median_x_Ankle\",\n",
    "    \"Med wrist pos x\": \"median_x_Wrist\",\n",
    "    \"Med ankle pos y\": \"median_y_Ankle\",\n",
    "    \"Cross-corr elbow angle\": \"lrCorr_angle_Elbow\",\n",
    "    \"Cross-corr wrist pos\": \"lrCorr_x_Wrist\",\n",
    "    \"Mean knee angle\": \"mean_angle_Knee\",\n",
    "    \"Cross-corr ankle pos\": \"lrCorr_x_Ankle\",\n",
    "    \"IQR wrist pos x\": \"IQR_x_Wrist\",\n",
    "    \"Cross-corr knee angle\": \"lrCorr_angle_Knee\",\n",
    "    \"IQR wrist pos y\": \"IQR_y_Wrist\",\n",
    "    \"Entropy wrist pos\": \"mean_ent_Wrist\",\n",
    "    \"Med wrist vel x\": \"median_vel_x_Wrist\",\n",
    "    \"IQR wrist vel x\": \"IQR_vel_x_Wrist\",\n",
    "    \"IQR wrist accel x\": \"IQR_acc_x_Wrist\",\n",
    "    \"Med wrist vel y\": \"median_vel_y_Wrist\",\n",
    "    \"IQR wrist vel y\": \"IQR_vel_y_Wrist\",\n",
    "    \"IQR wrist accel y\": \"IQR_acc_y_Wrist\",\n",
    "    \"Stdev elbow angle\": \"stdev_angle_Elbow\",\n",
    "    \"IQR elbow angle accel\": \"IQR_acc_Elbow\",\n",
    "    \"Entropy elbow angle\": \"entropy_angle_Elbow\",\n",
    "    \"IQR elbow angle vel\": \"IQR_vel_Elbow\",\n",
    "    \"Med elbow angle vel\": \"median_vel_Elbow\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(f'{pose_estimate_path}/window_mean_by_feature/final', exist_ok=True)\n",
    "\n",
    "for file, feature in feature_to_file_mapping.items():\n",
    "    #print(f'now renaming {feature} to {file}')\n",
    "    new_feature_name = file.replace(' ', '_')\n",
    "    # print(f'renaming {feature} to {new_feature_name}')\n",
    "    shutil.copy(f'{pose_estimate_path}/window_mean_by_feature/{feature}.csv', f'{pose_estimate_path}/window_mean_by_feature/final/{new_feature_name}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mapping = {\n",
    "    \"IQR_x_Ankle\": \"IQR_ankle_pos_x\",\n",
    "    \"IQRx_Ankle\": \"IQR_ankle_pos_x\",\n",
    "    \"median_vel_x_Ankle\": \"Med_ankle_vel_x\",\n",
    "    \"medianvelx_Ankle\": \"Med_ankle_vel_x\",\n",
    "    \"IQR_vel_x_Ankle\": \"IQR_ankle_vel_x\",\n",
    "    \"IQRvelx_Ankle\": \"IQR_ankle_vel_x\",\n",
    "    \"mean_ent_Ankle\": \"Entropy_ankle_pos\",\n",
    "    \"meanent_Ankle\": \"Entropy_ankle_pos\",\n",
    "    \"IQR_acc_x_Ankle\": \"IQR_ankle_accel_x\",\n",
    "    \"IQRaccx_Ankle\": \"IQR_ankle_accel_x\",\n",
    "    \"median_vel_y_Ankle\": \"Med_ankle_vel_y\",\n",
    "    \"medianvely_Ankle\": \"Med_ankle_vel_y\",\n",
    "    \"IQR_vel_y_Ankle\": \"IQR_ankle_vel_y\",\n",
    "    \"IQRvely_Ankle\": \"IQR_ankle_vel_y\",\n",
    "    \"median_vel_Knee\": \"Med_knee_angle_vel\",\n",
    "    \"median_vel_angle_Knee\": \"Med_knee_angle_vel\",\n",
    "    \"IQR_vel_Knee\": \"IQR_knee_angle_vel\",\n",
    "    \"IQR_vel_angle_Knee\": \"IQR_knee_angle_vel\",\n",
    "    \"median_y_Wrist\": \"Med_wrist_pos_y\",\n",
    "    \"mediany_Wrist\": \"Med_wrist_pos_y\",\n",
    "    \"IQR_y_Ankle\": \"IQR_ankle_pos_y\",\n",
    "    \"IQRy_Ankle\": \"IQR_ankle_pos_y\",\n",
    "    \"entropy_angle_Knee\": \"Entropy_knee_angle\",\n",
    "    \"IQR_acc_Knee\": \"IQR_knee_angle_accel\",\n",
    "    \"IQR_acc_angle_Knee\": \"IQR_knee_angle_accel\",\n",
    "    \"mean_angle_Elbow\": \"Mean_elbow_angle\",\n",
    "    \"IQR_acc_y_Ankle\": \"IQR_ankle_accel_y\",\n",
    "    \"IQRaccy_Ankle\": \"IQR_ankle_accel_y\",\n",
    "    \"median_x_Ankle\": \"Med_ankle_pos_x\",\n",
    "    \"medianx_Ankle\": \"Med_ankle_pos_x\",\n",
    "    \"median_x_Wrist\": \"Med_wrist_pos_x\",\n",
    "    \"medianx_Wrist\": \"Med_wrist_pos_x\",\n",
    "    \"median_y_Ankle\": \"Med_ankle_pos_y\",\n",
    "    \"mediany_Ankle\": \"Med_ankle_pos_y\",\n",
    "    \"lrCorr_angle_Elbow\": \"Cross-corr_elbow_angle\",\n",
    "    \"lrCorr_angle_Knee\": \"Cross-corr_knee_angle\",\n",
    "    \"lrCorr_x_Wrist\": \"Cross-corr_wrist_pos\",\n",
    "    \"mean_angle_Knee\": \"Mean_knee_angle\",\n",
    "    \"lrCorr_x_Ankle\": \"Cross-corr_ankle_pos\",\n",
    "    \"IQR_x_Wrist\": \"IQR_wrist_pos_x\",\n",
    "    \"IQRx_Wrist\": \"IQR_wrist_pos_x\",\n",
    "    \"IQR_y_Wrist\": \"IQR_wrist_pos_y\",\n",
    "    \"IQRy_Wrist\": \"IQR_wrist_pos_y\",\n",
    "    \"mean_ent_Wrist\": \"Entropy_wrist_pos\",\n",
    "    \"meanent_Wrist\": \"Entropy_wrist_pos\",\n",
    "    \"median_vel_x_Wrist\": \"Med_wrist_vel_x\",\n",
    "    \"medianvelx_Wrist\": \"Med_wrist_vel_x\",\n",
    "    \"IQR_vel_x_Wrist\": \"IQR_wrist_vel_x\",\n",
    "    \"IQRvelx_Wrist\": \"IQR_wrist_vel_x\",\n",
    "    \"IQR_acc_x_Wrist\": \"IQR_wrist_accel_x\",\n",
    "    \"IQRaccx_Wrist\": \"IQR_wrist_accel_x\",\n",
    "    \"median_vel_y_Wrist\": \"Med_wrist_vel_y\",\n",
    "    \"medianvely_Wrist\": \"Med_wrist_vel_y\",\n",
    "    \"IQR_vel_y_Wrist\": \"IQR_wrist_vel_y\",\n",
    "    \"IQRvely_Wrist\": \"IQR_wrist_vel_y\",\n",
    "    \"IQR_acc_y_Wrist\": \"IQR_wrist_accel_y\",\n",
    "    \"IQRaccy_Wrist\": \"IQR_wrist_accel_y\",\n",
    "    \"stdev_angle_Elbow\": \"Stdev_elbow_angle\",\n",
    "    \"stdev_angle_Knee\": \"Stdev_knee_angle\",\n",
    "    \"IQR_acc_Elbow\": \"IQR_elbow_angle_accel\",\n",
    "    \"IQR_acc_angle_Elbow\": \"IQR_elbow_angle_accel\",\n",
    "    \"entropy_angle_Elbow\": \"Entropy_elbow_angle\",\n",
    "    \"IQR_vel_Elbow\": \"IQR_elbow_angle_vel\",\n",
    "    \"IQR_vel_angle_Elbow\": \"IQR_elbow_angle_vel\",\n",
    "    \"median_vel_Elbow\": \"Med_elbow_angle_vel\",\n",
    "    \"median_vel_angle_Elbow\": \"Med_elbow_angle_vel\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(f'{pose_estimate_path}/window_mean_by_feature/final', exist_ok=True)\n",
    "\n",
    "for file, feature in feature_to_file_mapping.items():\n",
    "    #print(f'now renaming {feature} to {file}')\n",
    "    new_feature_name = file.replace(' ', '_')\n",
    "    # print(f'renaming {feature} to {new_feature_name}')\n",
    "    shutil.copy(f'{pose_estimate_path}/window_mean_by_feature/{feature}.csv', f'{pose_estimate_path}/window_mean_by_feature/final/{new_feature_name}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_feature_file = 'pose_estimates/gma_score_prediction_pose_estimates/features_mean_by_side.csv'\n",
    "total_features = pd.read_csv(total_feature_file)\n",
    "\n",
    "total_features['feature_part'] = total_features['feature'].astype(str) + '_' + total_features['part'].astype(str)\n",
    "unique_combinations = total_features['feature_part'].unique()\n",
    "\n",
    "unique_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_feature_file = 'pose_estimates/gma_score_prediction_pose_estimates/features_mean_by_side.csv'\n",
    "total_features = pd.read_csv(total_feature_file)\n",
    "\n",
    "total_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in total_features['feature_part'].unique():\n",
    "    try: \n",
    "        print(f'{feature} --> {feature_mapping[feature]}')\n",
    "    except KeyError: \n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features_total = pd.DataFrame()\n",
    "\n",
    "for feature in total_features['feature_part'].unique():\n",
    "\n",
    "    try: \n",
    "        new_feature_name = feature_mapping[feature]\n",
    "    except KeyError: \n",
    "        continue\n",
    "\n",
    "    slice = total_features[total_features['feature_part'] == feature]\n",
    "    slice['new_feature'] = new_feature_name\n",
    "\n",
    "    # Properly dropping columns and renaming with assignment\n",
    "    slice = slice.drop(columns=['feature', 'feature_part','age','session'])  # Assuming 'feauture' is a typo and should be 'feature'\n",
    "    slice = slice.rename(columns={'new_feature': 'feature'})\n",
    "    \n",
    "    if not slice.empty:\n",
    "     final_features_total = pd.concat([final_features_total, slice])\n",
    "\n",
    "display(final_features_total)\n",
    "\n",
    "print(f'total number of features: {len(final_features_total.feature.unique())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features_total.to_csv(f'{pose_estimate_path}/all_features_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the files in the window_mean_by_feature/final folder into one file\n",
    "final_features = pd.DataFrame()\n",
    "for file in os.listdir(f'{pose_estimate_path}/window_mean_by_feature/final'):\n",
    "    df = pd.read_csv(f'{pose_estimate_path}/window_mean_by_feature/final/{file}')\n",
    "    df['feature'] = file.replace('.csv', '')\n",
    "    df.rename(columns={'video': 'infant'}, inplace=True)\n",
    "    #write iteratively to a csv file\n",
    "    # if the file doesn't exist, write the header\n",
    "    if not os.path.exists(f'{pose_estimate_path}/window_mean_by_feature/final/all_features.csv'):\n",
    "        df.to_csv(f'{pose_estimate_path}/window_mean_by_feature/final/all_features.csv', header=True, index=False)\n",
    "    else:\n",
    "        df.to_csv(f'{pose_estimate_path}/window_mean_by_feature/final/all_features.csv', mode='a', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window_features = pd.read_csv(f'{pose_estimate_path}/window_mean_by_feature/final/all_features.csv')\n",
    "windows_mean = window_features.groupby(['infant','feature','part']).mean().drop(columns='frame').reset_index()\n",
    "display(windows_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_features_final = windows_mean.pivot_table(index=['infant'], columns='feature', values='Value', aggfunc='mean').reset_index() \n",
    "windows_features_final = windows_features_final.reset_index().rename_axis(None, axis=1)\n",
    "windows_features_final = windows_features_final.drop(columns='index')\n",
    "windows_features_final.head()\n",
    "\n",
    "windows_features_final.to_csv(f'{pose_estimate_path}/window_mean_by_feature/final/final_window_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features_total = pd.read_csv(f'{pose_estimate_path}/all_features_total.csv')\n",
    "final_features_total = final_features_total.pivot_table(index=['infant'], columns='feature', values='Value', aggfunc='mean').reset_index()\n",
    "final_features_total  = final_features_total .reset_index().rename_axis(None, axis=1)\n",
    "final_features_total  = final_features_total .drop(columns='index')\n",
    "\n",
    "final_features_total.to_csv(f'{pose_estimate_path}/final_total_features.csv')\n",
    "final_features_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in windows_mean.feature.unique():\n",
    "    if feature not in final_features_total.feature.unique():\n",
    "        print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in final_features_total.feature.unique():\n",
    "    if feature not in windows_mean.feature.unique():\n",
    "        print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_features_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(windows_features_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make subplots for each column in the final features dataframes comparing windowed and total features\n",
    "\n",
    "# Get the list of features\n",
    "\n",
    "features = final_features_total.columns[1:]  # Exclude 'infant' column\n",
    "\n",
    "# Create a figure and subplots\n",
    "n_cols = 3  # Number of columns in the grid\n",
    "n_rows = -(-len(features) // n_cols)  # Ceiling division for the number of rows\n",
    "plt.figure(figsize=(20, 5*n_rows))\n",
    "\n",
    "# Plot each feature in its subplot with a correlation coefficient\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    # comput the correlation between total and windows \n",
    "    r = final_features_total[feature].corr(windows_features_final[feature])\n",
    "\n",
    "    plt.subplot(n_rows, n_cols, i+1)\n",
    "    plt.scatter(final_features_total[feature], windows_features_final[feature], alpha=0.5)\n",
    "    plt.title(f'{feature}: R2 = {r:.2f}')\n",
    "    plt.xlabel('Total Feature')\n",
    "    plt.ylabel('Window Feature')\n",
    "    plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "predict_gma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
